AWSTemplateFormatVersion: 2010-09-09
Description: "CloudFormation Template to deploy an ETL orchestration pipeline by using Redshift Data Api, Step Function and AWS Lambda"
Parameters:
  RedshiftClusterEndpoint:
    Description: Redshift cluster endpoint including port number and database name
    Type: String
    Default: redshift-cluster.xxxxxx.region.redshift.amazonaws.com:5439/dev
  DbUsername:
    Description: Redshift database user name which has access to run SQL Script.
    Type: String
    AllowedPattern: "([a-z])([a-z]|[0-9])*"
    Default: 'awsuser'
  S3BucketNamePublic:
    Description: 'S3 bucket name that will store all assets of this solution: cloud formation templates and SQL scripts that will be copied during deployment.'
    Type: String
    Default: 'YourS3BucketName'
  S3BucketNameLocal:
    Description: S3 bucket name (and folder if needed) to store review SQL script files. Please note, this automation would grant full access on your SQL script files' S3 bucket.
    Type: String
    Default: 'YourS3BucketName'
  ScripPath:
    Description: A path in the S3BucketNameLocal for SQL scripts.
    Type: String
    Default: 'scripts'
  ResultPath:
    Description: A path in the S3BucketNameLocal for result tables.
    Type: String
    Default: 'result'
Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: Input Parameters
        Parameters:
          - S3BucketNamePublic
          - S3BucketNameLocal
          - RedshiftClusterEndpoint
          - DbUsername
          - ScriptPath
          - ResultPath
Mappings:
  Script: # static values related to the scripts.
    Config:
      NodeDetails: 'rpr_NodeDetails.sql'
      QueueHealth: 'rpr_QueueHealth.sql'
      AlterTableRecommendation: 'rpr_AlterTableRecommendation.sql'
      SpectrumPerformance: 'rpr_SpectrumPerformance.sql'
      ATOWorkerActions: 'rpr_ATOWorkerActions.sql'
      ConcurrencyScalingUsage: 'rpr_ConcurrencyScalingUsage.sql'
      CopyPerformance: 'rpr_CopyPerformance.sql'
      DataShareConsumerUsage: 'rpr_DataShareConsumerUsage.sql'
      DataShareProducerObject: 'rpr_DataShareProducerObject.sql'
      MaterializedView: 'rpr_MaterializedView.sql'
      TableAlerts: 'rpr_TableAlerts.sql'
      Tablelnfo: 'rpr_Tablelnfo.sql'
      Top50QueriesByDiskSpill: 'rpr_Top50QueriesByDiskSpill.sql'
      Top50QueriesByRunTime: 'rpr_Top50QueriesByRunTime.sql'
      UsagePattern: 'rpr_UsagePattern.sql'
      WLMandCommit: 'rpr_WLMandCommit.sql'
      WLMConfig: 'rpr_WLMConfig.sql'
      WorkloadEvaluation: 'rpr_WorkloadEvaluation.sql'

Resources:
# create here three nested stacks
  NestedStackS3Buckets:
    Type: AWS::CloudFormation::Stack
    Properties:

      TemplateURL: !Sub 'https://s3.amazonaws.com/${S3BucketNameGlobal}/stacks/nested-stack-s3-buckets.yaml'
      Parameters:
        S3BucketNameLocal: !Ref S3BucketNameLocal
        ScriptPath: !Ref ScriptPath
        ResultPath: !Ref ResultPath

  NestedStackStepFunctions:
    Type: AWS::CloudFormation::Stack
    Properties:
      TemplateURL: !Sub 'https://s3.amazonaws.com/${S3BucketNameGlobal}/stacks/nested-stack-step-functions.yaml'
      Parameters:
        RedshiftClusterEndpoint: !Ref RedshiftClusterEndpoint
        DbUsername: !Ref DbUsername
        S3BucketNameLocal: !Ref S3BucketNameLocal
        ScriptPath: !Ref ScriptPath
        ResultPath: !Ref ResultPath

  StateMachineExecutionRole:
    Type: 'AWS::IAM::Role'
    Properties:
      Description : IAM Role for the state machine in step function to run
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - !Sub 'states.${AWS::Region}.amazonaws.com'
            Action: 'sts:AssumeRole'
      Path: /
      Policies:
        - PolicyName: RedshiftBatchDataApiPolicy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - 'redshift-data:BatchExecuteStatement'
                  - 'redshift-data:ListStatements'
                  - 'redshift-data:GetStatementResult'
                  - 'redshift-data:DescribeStatement'
                  - 'redshift-data:ExecuteStatement'
                Resource: '*'
              - Effect: Allow
                Action:
                  - 'redshift:GetClusterCredentials'
                Resource:
                  - !Sub 
                      - arn:aws:redshift:${AWS::Region}:${AWS::AccountId}:cluster:${SourceRedshiftClusterIdentifier}
                      - {SourceRedshiftClusterIdentifier: !Select [0, !Split [".", !Ref RedshiftClusterEndpoint]]}
                  - !Sub 
                      - arn:aws:redshift:${AWS::Region}:${AWS::AccountId}:dbname:${SourceRedshiftClusterIdentifier}/${RedshiftDatabaseName}
                      - {SourceRedshiftClusterIdentifier: !Select [0, !Split [".", !Ref RedshiftClusterEndpoint]],RedshiftDatabaseName: !Select [1, !Split ["/", !Ref RedshiftClusterEndpoint]]}   
                  - !Sub 
                      - arn:aws:redshift:${AWS::Region}:${AWS::AccountId}:dbuser:${SourceRedshiftClusterIdentifier}/${DbUsername}
                      - {SourceRedshiftClusterIdentifier: !Select [0, !Split [".", !Ref RedshiftClusterEndpoint]]}
        - PolicyName: ScriptS3AccessPolicy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - 's3:GetBucketLocation'
                  - 's3:GetObject'
                  - 's3:ListBucket'
                Resource: 
                  - !Sub
                      - arn:aws:s3:::${ETLScriptS3Bucket}/*
                      - {ETLScriptS3Bucket: !Select [0, !Split ["/", !Ref ETLScriptS3Path]]}
                  - !Sub
                      - arn:aws:s3:::${ETLScriptS3Bucket}
                      - {ETLScriptS3Bucket: !Select [0, !Split ["/", !Ref ETLScriptS3Path]]}

  RedshiftReviewStepFunction:
    Type: 'AWS::StepFunctions::StateMachine'
    Properties:
      DefinitionString: !Sub 
        - |-   
          {
            "Comment": "A simple ETL workflow for loading dimension and fact tables",
          
          }
        - RedshiftClusterIdentifier: !Select [0, !Split [".", !Ref RedshiftClusterEndpoint]]
          RedshiftDbName: !Select [1, !Split ["/", !Ref RedshiftClusterEndpoint]]
          RedshiftDbUser: !Ref DbUsername
          S3BucketName: !Ref ETLScriptS3Path
          SetupScriptFilename: !FindInMap [Script, Config, SetupScript]
          ValidateScriptFilename: !FindInMap [Script, Config, ValidateScript]       
      RoleArn: !GetAtt StateMachineExecutionRole.Arn

  LambdaInvokeStepFunctionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "LambdaInvokeStepFunctionRole-${AWS::AccountId}"
      Description: IAM Role for lambda to execute the Step Function
      AssumeRolePolicyDocument:
          Version: 2012-10-17
          Statement:
            -
              Effect: Allow
              Principal:
                Service:
                  - lambda.amazonaws.com
              Action:
                - sts:AssumeRole
      Path: /
      Policies:
          -
            PolicyName: LambdaInvokePolicy
            PolicyDocument :
              Version: 2012-10-17
              Statement:
                -
                  Effect: "Allow"
                  Action:
                    - states:StartExecution
                  Resource: !Ref RedshiftETLStepFunction
          -
            PolicyName: LambdaCloudFormationPolicy
            PolicyDocument:
              Version: 2012-10-17
              Statement:
                -
                  Effect: Allow
                  Action:
                    - s3:*
                  Resource:
                    - !Sub "arn:aws:s3:::cloudformation-custom-resource-response-${AWS::Region}"
                    - !Sub "arn:aws:s3:::cloudformation-waitcondition-${AWS::Region}"
                    - !Sub "arn:aws:s3:::cloudformation-custom-resource-response-${AWS::Region}/*"
                    - !Sub "arn:aws:s3:::cloudformation-waitcondition-${AWS::Region}/*"

  LambdaInvokeStepFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub "LambdaInvokeStepFunction-${AWS::AccountId}"
      Description: Lambda to execute the step function
      Handler: index.handler
      Runtime: python3.8
      Role: !GetAtt 'LambdaInvokeStepFunctionRole.Arn'
      Timeout: 60
      Code:
        ZipFile: |
          import boto3
          import traceback
          import json
          import cfnresponse
          def handler(event, context):
            print(event)
            step_function_client = boto3.client('stepfunctions')
            res = {}
            S3BucketName = event['ResourceProperties'].get('S3BucketNameLocal')
            step_function_input =  {
              "Task" : "NodeDetails",
              "Workflow": {
                "NodeDetails": {
                      "ScriptName" : "rpr_NodeDetails.sql",
                      "OutputLocation" : f"s3://{S3BucketName}/result/node_details.csv"
                      "TimeThreshold" : "60",
                      "Status": "INITIAL",
                      },
                "QueueHealth": {
                      "ScriptName" : "rpr_QueueHealth.sql",
                      "OutputLocation" :  lower(f"s3://{S3BucketName}/result/queue_health.csv")
                      "TimeThreshold" : "60",
                      "Status": "INITIAL",
                      },
                "AlterTableRecommendation": {
                      "ScriptName" : "rpr_AlterTableRecommendation.sql",
                      "OutputLocation" :  f"s3://{S3BucketName}/result/alter_table_recommendation.csv"
                      "TimeThreshold" : "60",
                      "Status": "INITIAL",
                      },
                "SpectrumPerformance": {
                      "ScriptName" : "rpr_SpectrumPerformance.sql",
                      "OutputLocation" :  f"s3://{S3BucketName}/result/spectrum_performance.csv"
                      "TimeThreshold" : "60",
                      "Status": "INITIAL",
                      },
                "ATOWorkerActions": {
                      "ScriptName" : "rpr_ATOWorkerActions.sql",
                      "OutputLocation" :  f"s3://{S3BucketName}/result/ato_worker_actions.csv"
                      "TimeThreshold" : "60",
                      "Status": "INITIAL",
                      },
                "ConcurrencyScalingUsage": {
                      "ScriptName" : "rpr_ConcurrencyScalingUsage.sql",
                      "OutputLocation" :  f"s3://{S3BucketName}/result/concurrency_scaling_usage.csv"
                      "TimeThreshold" : "60",
                      "Status": "INITIAL",
                },
                "CopyPerformance": {
                      "ScriptName" : "rpr_CopyPerformance.sql",
                      "OutputLocation" :  f"s3://{S3BucketName}/result/copy_performance.csv"
                      "TimeThreshold" : "60",
                      "Status": "INITIAL",
                      },
                "DataShareConsumerUsage": {
                      "ScriptName" : "rpr_DataShareConsumerUsage.sql",
                      "OutputLocation" :  f"s3://{S3BucketName}/result/data_share_consumer_usage.csv"
                      "TimeThreshold" : "60",
                      "Status": "INITIAL",
                      },
                "DataShareProducerObject": {
                      "ScriptName" : "rpr_DataShareProducerObject.sql",
                      "OutputLocation" :  f"s3://{S3BucketName}/result/data_share_producer_object.csv"
                      "TimeThreshold" : "60",
                      "Status": "INITIAL",
                      },
                "MaterializedView": {
                      "ScriptName" : "rpr_MaterializedView.sql",
                      "OutputLocation" :  f"s3://{S3BucketName}/result/materialized_view.csv"
                      "TimeThreshold" : "60",
                      "Status": "INITIAL",
                      },
                "TableAlerts": {
                      "ScriptName" : "rpr_TableAlerts.sql",
                      "OutputLocation" :  f"s3://{S3BucketName}/result/table_alerts.csv"
                      "TimeThreshold" : "60",
                      "Status": "INITIAL",
                      },
                "TableInfo":  {
                      "ScriptName" : "rpr_TableInfo.sql",
                      "OutputLocation" :  f"s3://{S3BucketName}/result/table_info.csv"
                      "TimeThreshold" : "60",
                      "Status": "INITIAL",
                      },
                "Top50QueriesByDiskSpill" : {
                      "ScriptName" : "rpr_Top50QueriesByDiskSpill.sql",
                      "OutputLocation" :  f"s3://{S3BucketName}/result/top_50_queries_by_disk_spill.csv"
                      "TimeThreshold" : "60",
                      "Status": "INITIAL",
                      },
                "Top50QueriesByRunTime": {
                      "ScriptName" : "rpr_Top50QueriesByRunTime.sql",
                      "OutputLocation" :  f"s3://{S3BucketName}/result/top_50_queries_by_run_time.csv"
                      "TimeThreshold" : "60",
                      "Status": "INITIAL",
                      }, 
                "UsagePattern": {
                      "ScriptName" : "rpr_UsagePattern.sql",
                      "OutputLocation" :  f"s3://{S3BucketName}/result/usage_pattern.csv"
                      "TimeThreshold" : "60",
                      "Status": "INITIAL",
                    },
                "WaitTimeAndCount": {
                      "ScriptName" : "rpr_WaitTimeAndCount.sql",
                      "OutputLocation" :  f"s3://{S3BucketName}/result/wait_time_and_count.csv"
                      "TimeThreshold" : "60",
                      "Status": "INITIAL",
                      },
                }',
                "WLMandCommit": {
                      "ScriptName" : "rpr_WLMandCommit.sql",
                      "OutputLocation" :  f"s3://{S3BucketName}/result/wlm_and_commit.csv"
                      "TimeThreshold" : "60",
                      "Status": "INITIAL",
                      }, 
                "WLMConfig": {
                      "ScriptName" : "rpr_WLMConfig.sql",
                      "OutputLocation" :  f"s3://{S3BucketName}/result/wlm_config.csv"
                      "TimeThreshold" : "60",
                      "Status": "INITIAL",
                      },
                "WorkloadEvaluation" : {
                      "ScriptName" : "rpr_WorkloadEvaluation.sql",
                      "OutputLocation" :  f"s3://{S3BucketName}/result/workload_evaluation.csv"
                      "TimeThreshold" : "60",
                      "Status": "INITIAL",
                      },  
                }}
            if event['RequestType'] != 'Delete':
                try:
                    step_function_input = {"comment": "Execute ETL Workflow for Redshift"}
                    response = step_function_client.start_execution(stateMachineArn=event['ResourceProperties'].get('StepFunctionArn'),
                                                                    input=json.dumps(step_function_input)
                                                                  )
                    print(response)
                except:
                    print(traceback.format_exc())
                    cfnresponse.send(event, context, cfnresponse.FAILED, input)
                    raise
            cfnresponse.send(event, context, cfnresponse.SUCCESS, res)

  StartStepFunction:
    Type: Custom::LambdaStartStepFunction
    Properties:
      ServiceToken: !GetAtt [LambdaInvokeStepFunction, Arn]
      StepFunctionArn: !Ref RedshiftETLStepFunction
Outputs:
  RedshiftETLStepFunctionArn:
    Description: "The ARN of the step function used for ETL orchestration"
    Value:
      Ref: RedshiftETLStepFunction
