{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7a91a8-ea50-4d5b-9914-221d28a30e1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SQL Query Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db389308-b31d-4158-ba2d-86cff3117589",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a08e96a-afe6-47cf-91e3-33ac5748f79b",
   "metadata": {},
   "source": [
    "In this notebook we show the differences between the Bedrock large language models, as well as Q, when they are use to analyze the SQL queries. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3a5fd2-b386-4338-b198-de6aab11258c",
   "metadata": {},
   "source": [
    "### Queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98673af0-4f96-4a82-a1ea-c1be8781c66f",
   "metadata": {
    "tags": []
   },
   "source": [
    "For our test set we use the queries that are included in the Redshift Cluster review. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e68f9-76f4-45f5-8dd0-3707b5639178",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8bb44c3-fa20-4da8-b1ab-f1cb7ebefebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/LifeSaver/LocalProjects/amazon-bedrock-samples/redshift_review_data/test\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ad3004-1fe9-46c5-8661-e8e5e00f3b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3576d29-28e0-4b41-931a-7b6398515aa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries[\"NodeDetails\"] = {}\n",
    "queries[\"NodeDetails\"][\"text\"] = \"\"\"WITH node_slice AS\n",
    "(\n",
    "  SELECT node,\n",
    "         COUNT(1) AS slice_count\n",
    "  FROM stv_slices\n",
    "  GROUP BY node\n",
    "),\n",
    "node_storage_utilization AS\n",
    "(\n",
    "  SELECT node::text AS node,\n",
    "         (1.0 * used / capacity)::NUMERIC(8,4) * 100 AS storage_utilization_pct,\n",
    "         1.0 * capacity/1000 AS storage_capacity_gb,\n",
    "         1.0 * used/1000 AS storage_used_gb\n",
    "  FROM stv_node_storage_capacity\n",
    ")\n",
    "SELECT CASE\n",
    "         WHEN capacity = 190633 AND NOT is_nvme THEN 'dc1.large'\n",
    "         WHEN capacity = 380319 THEN 'dc1.8xlarge'\n",
    "         WHEN capacity = 190633 AND is_nvme THEN 'dc2.large'\n",
    "         WHEN capacity = 760956 THEN 'dc2.8xlarge'\n",
    "         WHEN capacity = 726296 THEN 'dc2.8xlarge'\n",
    "         WHEN capacity = 952455 THEN 'ds2.xlarge'\n",
    "         WHEN capacity = 945026 THEN 'ds2.8xlarge'\n",
    "         WHEN capacity = 954367 AND part_count = 1 THEN 'ra3.xlplus'\n",
    "         WHEN capacity = 3339176 AND part_count = 1 THEN 'ra3.4xlarge'\n",
    "         WHEN capacity = 3339176 AND part_count = 4 THEN 'ra3.16xlarge'\n",
    "         ELSE 'unknown'\n",
    "       END AS node_type,\n",
    "       s.node,\n",
    "       slice_count,\n",
    "       storage_utilization_pct,\n",
    "       storage_capacity_gb,\n",
    "       storage_used_gb\n",
    "FROM (SELECT p.host AS node,\n",
    "             p.capacity,\n",
    "             p.mount LIKE '/dev/nvme%' AS is_nvme,\n",
    "             COUNT(1) AS part_count\n",
    "      FROM stv_partitions p\n",
    "      WHERE p.host = p.owner\n",
    "      GROUP BY 1,\n",
    "               2,\n",
    "               3) AS s\n",
    "  INNER JOIN node_slice n ON (s.node = n.node)\n",
    "  INNER JOIN node_storage_utilization ns ON (s.node = ns.node)\n",
    "ORDER by 2;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "130e9bc0-51bd-400e-a550-6f2f78075e7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries[\"WLMConfig\"] = {}\n",
    "queries[\"WLMConfig\"][\"text\"] = \"\"\"SELECT c.wlm_mode,\n",
    "       scc.service_class::text AS service_class_id,\n",
    "       CASE\n",
    "         WHEN scc.service_class BETWEEN 1 AND 4 THEN 'System'\n",
    "         WHEN scc.service_class = 5 THEN 'Superuser'\n",
    "         WHEN scc.service_class BETWEEN 6 AND 13 THEN 'Manual WLM'\n",
    "         WHEN scc.service_class = 14 THEN 'SQA'\n",
    "         WHEN scc.service_class = 15 THEN 'Redshift Maintenance'\n",
    "         WHEN scc.service_class BETWEEN 100 AND 107 THEN 'Auto WLM'\n",
    "       END AS service_class_category,\n",
    "       trim(scc.name) AS queue_name,\n",
    "       CASE\n",
    "         WHEN scc.num_query_tasks = -1 THEN 'auto'\n",
    "         ELSE scc.num_query_tasks::text\n",
    "       END AS slots,\n",
    "       CASE\n",
    "         WHEN scc.query_working_mem = -1 THEN 'auto'\n",
    "         ELSE scc.query_working_mem::text\n",
    "       END AS query_working_memory_mb_per_slot,\n",
    "       nvl(cast(ROUND(((scc.num_query_tasks*scc.query_working_mem)::NUMERIC/ mem.total_memory_mb::NUMERIC)*100,0)::NUMERIC(38,4) as varchar(12)),'auto') cluster_memory_pct,\n",
    "       scc.max_execution_time AS query_timeout,\n",
    "       trim(scc.concurrency_scaling) AS concurrency_scaling,\n",
    "       trim(scc.query_priority) AS queue_priority,\n",
    "       nvl(qc.qmr_rule_count,0) AS qmr_rule_count,\n",
    "       CASE\n",
    "         WHEN qmr.qmr_rule IS NOT NULL THEN 'Y'\n",
    "         ELSE 'N'\n",
    "       END AS is_queue_evictable,\n",
    "       LISTAGG(DISTINCT TRIM(qmr.qmr_rule),',') within group(ORDER BY rule_name) qmr_rule,\n",
    "       LISTAGG(TRIM(cnd.condition),', ') condition\n",
    "FROM stv_wlm_service_class_config scc\n",
    "  INNER JOIN stv_wlm_classification_config cnd ON scc.service_class = cnd.action_service_class\n",
    "  CROSS JOIN (SELECT CASE\n",
    "                       WHEN COUNT(1) > 0 THEN 'auto'\n",
    "                       ELSE 'manual'\n",
    "                     END AS wlm_mode\n",
    "              FROM stv_wlm_service_class_config\n",
    "              WHERE service_class >= 100) c\n",
    "  CROSS JOIN (SELECT SUM(num_query_tasks*query_working_mem) AS total_memory_mb\n",
    "              FROM stv_wlm_service_class_config\n",
    "              WHERE service_class BETWEEN 6 AND 13) mem\n",
    "  LEFT OUTER JOIN (SELECT service_class,\n",
    "                          COUNT(DISTINCT rule_name) AS qmr_rule_count\n",
    "                   FROM stv_wlm_qmr_config\n",
    "                   GROUP BY service_class) qc ON (scc.service_class = qc.service_class)\n",
    "  LEFT OUTER JOIN (SELECT service_class,\n",
    "                          rule_name,\n",
    "                          rule_name || ':' || '[' || action || '] ' || metric_name || metric_operator || CAST(metric_value AS VARCHAR(256)) qmr_rule\n",
    "                   FROM stv_wlm_qmr_config) qmr ON scc.service_class = qmr.service_class\n",
    "WHERE scc.service_class > 4\n",
    "GROUP BY 1,\n",
    "         2,\n",
    "         3,\n",
    "         4,\n",
    "         5,\n",
    "         6,\n",
    "         7,\n",
    "         8,\n",
    "         9,\n",
    "         10,\n",
    "         11,\n",
    "         12\n",
    "ORDER BY 2 ASC;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26fafa31-1056-48ce-bd96-11c21c9f06a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries[\"WLMandCommit\"] = {}\n",
    "queries[\"WLMandCommit\"][\"text\"] = \"\"\"SELECT IQ.*,\n",
    "       (IQ.wlm_queue_time_ms/ IQ.wlm_start_commit_time_ms)*100.0::numeric(6,2) AS pct_wlm_queue_time,\n",
    "       (IQ.exec_only_time_ms/ IQ.wlm_start_commit_time_ms)*100.0::numeric(6,2) AS pct_exec_only_time,\n",
    "       (IQ.commit_queue_time_ms/ IQ.wlm_start_commit_time_ms)*100.0::numeric(6,2) pct_commit_queue_time,\n",
    "       (IQ.commit_time_ms/ IQ.wlm_start_commit_time_ms)*100.0::numeric(6,2) pct_commit_time\n",
    "FROM (SELECT TRUNC(b.starttime) AS DAY,\n",
    "             d.service_class,\n",
    "             rtrim(s.name) as queue_name,\n",
    "             c.node,\n",
    "             COUNT(DISTINCT c.xid) AS count_commit_xid,\n",
    "             SUM(datediff ('microsec',d.service_class_start_time,c.endtime)*0.001)::numeric(38,4) AS wlm_start_commit_time_ms,\n",
    "             SUM(datediff ('microsec',d.queue_start_time,d.queue_end_time)*0.001)::numeric(38,4) AS wlm_queue_time_ms,\n",
    "             SUM(datediff ('microsec',b.starttime,b.endtime)*0.001)::numeric(38,4) AS exec_only_time_ms,\n",
    "             SUM(datediff ('microsec',c.startwork,c.endtime)*0.001)::numeric(38,4) commit_time_ms,\n",
    "             SUM(datediff ('microsec',DECODE(c.startqueue,'2000-01-01 00:00:00',c.startwork,c.startqueue),c.startwork)*0.001)::numeric(38,4) commit_queue_time_ms\n",
    "      FROM stl_query b,\n",
    "           stl_commit_stats c,\n",
    "           stl_wlm_query d,\n",
    "           stv_wlm_service_class_config s\n",
    "      WHERE b.xid = c.xid\n",
    "      AND   b.query = d.query\n",
    "      AND   c.xid > 0\n",
    "      AND d.service_class = s.service_class\n",
    "      GROUP BY 1,2,3,4\n",
    "      ORDER BY 1,2,3,4) IQ;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd3156b8-57ff-4fd1-b82f-636bd862e7f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries[\"QueueHealth\"] = {}\n",
    "queries[\"QueueHealth\"][\"text\"] = \"\"\"with workload as\n",
    "(\n",
    "select trim(sq.\"database\") as dbname\n",
    "      ,case \n",
    "\t     when sq.concurrency_scaling_status = 1 then 'burst'\t        \n",
    "\t\t\t else 'main' end as concurrency_scaling_status \n",
    "      ,case \n",
    "       when sl.source_query is not null then 'result_cache'       \n",
    "\t\t\t else rtrim(swsc.name) end as queue_name \n",
    "\t\t\t,swq.service_class \n",
    "      ,case\n",
    "       when swq.service_class between 1 and 4 then 'System'\n",
    "       when swq.service_class = 5 then 'Superuser'\n",
    "       when swq.service_class between 6 and 13 then'Manual WLM queues'\n",
    "       when swq.service_class = 14 then 'SQA'\n",
    "       when swq.service_class = 15 then 'Redshift Maintenance'\n",
    "       when swq.service_class between 100 and 107 then 'Auto WLM'\n",
    "       end as service_class_category \t  \n",
    "      ,sq.query as query_id\n",
    "      ,case \n",
    "         when regexp_instr(sq.querytxt, '(padb_|pg_internal)'             ) then 'OTHER'\n",
    "         when regexp_instr(sq.querytxt, '([uU][nN][dD][oO][iI][nN][gG]) ' ) then 'SYSTEM'\n",
    "         when regexp_instr (sq.querytxt,'([aA][uU][tT][oO][mM][vV])'      ) then 'AUTOMV'\n",
    "         when regexp_instr(sq.querytxt, '[uU][nN][lL][oO][aA][dD]'        ) then 'UNLOAD'\n",
    "         when regexp_instr(sq.querytxt, '[cC][uU][rR][sS][oO][rR] '       ) then 'CURSOR'\n",
    "         when regexp_instr(sq.querytxt, '[fF][eE][tT][cC][hH] '           ) then 'CURSOR'\n",
    "         WHEN regexp_instr (sq.querytxt,'[cC][rR][eE][aA][tT][eE] '       ) then 'CTAS'\n",
    "         when regexp_instr(sq.querytxt, '[dD][eE][lL][eE][tT][eE] '       ) then 'DELETE'\n",
    "         when regexp_instr(sq.querytxt, '[uU][pP][dD][aA][tT][eE] '       ) then 'UPDATE'\n",
    "         when regexp_instr(sq.querytxt, '[iI][nN][sS][eE][rR][tT] '       ) then 'INSERT'\n",
    "         when regexp_instr(sq.querytxt, '[vV][aA][cC][uU][uU][mM][ :]'    ) then 'VACUUM'\n",
    "         when regexp_instr(sq.querytxt, '[aA][nN][aA][lL][yY][zZ][eE] '   ) then 'ANALYZE'\t\t \n",
    "         when regexp_instr(sq.querytxt, '[sS][eE][lL][eE][cC][tT] '       ) then 'SELECT'\n",
    "         when regexp_instr(sq.querytxt, '[cC][oO][pP][yY] '               ) then 'COPY'\n",
    "         else 'OTHER' \n",
    "       end as query_type \n",
    "      ,date_trunc('hour',sq.starttime) as workload_exec_hour\n",
    "      ,nvl(swq.est_peak_mem/1024.0/1024.0/1024.0,0.0) as est_peak_mem_gb\n",
    "      ,decode(swq.final_state, 'Completed',decode(swr.action, 'abort',0,decode(sq.aborted,0,1,0)),'Evicted',0,null,decode(sq.aborted,0,1,0)::int) as is_completed\n",
    "      ,decode(swq.final_state, 'Completed',decode(swr.action, 'abort',1,0),'Evicted',1,null,0) as is_evicted_aborted\n",
    "      ,decode(swq.final_state, 'Completed',decode(swr.action, 'abort',0,decode(sq.aborted,1,1,0)),'Evicted',0,null,decode(sq.aborted,1,1,0)::int) as is_user_aborted\n",
    "\t    ,case when sl.from_sp_call is not null then 1 else 0 end as from_sp_call\n",
    "\t    ,case when alrt.num_events is null then 0 else alrt.num_events end as alerts\n",
    "\t    ,case when dsk.num_diskbased > 0 then 1 else 0 end as is_query_diskbased\n",
    "\t    ,nvl(c.num_compile_segments,0) as num_compile_segments\n",
    "      ,cast(case when sqms.query_queue_time is null then 0 else sqms.query_queue_time end as decimal(26,6)) as query_queue_time_secs\n",
    "\t    ,nvl(c.max_compile_time_secs,0) as max_compile_time_secs\n",
    "\t    ,sl.starttime\n",
    "\t    ,sl.endtime\n",
    "\t    ,sl.elapsed\n",
    "      ,cast(sl.elapsed * 0.000001 as decimal(26,6)) as query_execution_time_secs\t\n",
    "      ,sl.elapsed * 0.000001 - nvl(c.max_compile_time_secs,0)  - nvl(sqms.query_queue_time,0) as actual_execution_time_secs\t  \n",
    "      ,case when sqms.query_temp_blocks_to_disk is null then 0 else sqms.query_temp_blocks_to_disk end as query_temp_blocks_to_disk_mb\n",
    "      ,cast(case when sqms.query_cpu_time is null then 0 else sqms.query_cpu_time end as decimal(26,6)) as query_cpu_time_secs \n",
    "\t    ,nvl(sqms.scan_row_count,0) as scan_row_count\n",
    "      ,nvl(sqms.return_row_count,0) as return_row_count\n",
    "      ,nvl(sqms.nested_loop_join_row_count,0) as nested_loop_join_row_count\n",
    "      ,nvl(uc.usage_limit_count,0) as cs_usage_limit_count\n",
    "  from stl_query sq\n",
    "  inner join svl_qlog sl on (sl.userid = sq.userid and sl.query = sq.query)\n",
    "  left outer join svl_query_metrics_summary sqms on (sqms.userid = sq.userid and sqms.query = sq.query)\t\t\t\t\t\n",
    "  left outer join stl_wlm_query swq on (sq.userid = swq.userid and sq.query = swq.query)\n",
    "  left outer join stl_wlm_rule_action swr on (sq.userid = swr.userid and sq.query = swr.query and swq.service_class = swr.service_class)\n",
    "  left outer join stv_wlm_service_class_config swsc on (swsc.service_class = swq.service_class)\n",
    "  left outer join (select sae.query\n",
    "                         ,cast(1 as integer) as num_events\n",
    "                     from svcs_alert_event_log sae\n",
    "                   group by sae.query) as alrt on (alrt.query = sq.query)  \n",
    "  left outer join (select sqs.userid\n",
    "                         ,sqs.query\n",
    "                         ,1 as num_diskbased\n",
    "                     from svcs_query_summary sqs    \n",
    "                    where sqs.is_diskbased = 't'\n",
    "                   group by sqs.userid, sqs.query\n",
    "                   ) as dsk on (dsk.userid = sq.userid and dsk.query = sq.query)  \n",
    "  left outer join (select userid, xid,  pid, query\n",
    "                         ,max(datediff(ms, starttime, endtime)*1.0/1000) as max_compile_time_secs\n",
    "\t                     ,sum(compile) as num_compile_segments\n",
    "                     from svcs_compile\n",
    "                   group by userid, xid,  pid, query\n",
    "                  ) c on (c.userid = sq.userid and c.xid = sq.xid and c.pid = sq.pid and c.query = sq.query)                 \n",
    "  left outer join (select query,xid,pid\n",
    "                          ,count(1) as usage_limit_count\n",
    "                      from stl_usage_control \n",
    "                     where feature_type = 'CONCURRENCY_SCALING'\n",
    "                   group by query, xid, pid) uc on (uc.xid = sq.xid and uc.pid = sq.pid and uc.query = sq.query)                  \t   \n",
    "  where sq.userid <> 1 \n",
    "    and sq.querytxt not like 'padb_fetch_sample%'\n",
    "    and sq.starttime >= dateadd(day,-7,current_date)\n",
    ")\n",
    "select workload_exec_hour\n",
    "      ,service_class_category\n",
    "      ,service_class\n",
    "      ,queue_name\n",
    "      ,concurrency_scaling_status\n",
    "      ,dbname\n",
    "      ,query_type\n",
    "      ,sum(is_completed) + sum(is_user_aborted) + sum(is_evicted_aborted) as total_query_count\n",
    "      ,sum(is_completed) as completed_query_count\n",
    "      ,sum(is_user_aborted) as user_aborted_count\n",
    "      ,sum(is_evicted_aborted) as wlm_evicted_count\n",
    "      ,round(sum(est_peak_mem_gb),4) as total_est_peak_mem_gb\n",
    "      ,sum(is_query_diskbased) as total_disk_spill_count\n",
    "      ,sum(num_compile_segments) as total_compile_count\n",
    "      ,round(sum(query_temp_blocks_to_disk_mb/1024.0),4) as total_disk_spill_gb\n",
    "      ,sum(alerts) as total_query_alert_count\n",
    "      ,sum(from_sp_call) as total_called_proc_count\n",
    "\t  ,avg(query_execution_time_secs) as avg_query_execution_time_secs\n",
    "\t  ,max(query_execution_time_secs) as max_query_execution_time_secs\n",
    "      ,sum(query_execution_time_secs) as total_query_execution_time_secs\n",
    "\t  ,avg(max_compile_time_secs) as avg_compile_time_secs\n",
    "\t  ,max(max_compile_time_secs) as max_compile_time_secs\n",
    "      ,sum(max_compile_time_secs) as total_compile_time_secs\n",
    "\t  ,avg(query_queue_time_secs) as avg_query_queue_time_secs\n",
    "\t  ,max(query_queue_time_secs) as max_query_queue_time_secs\n",
    "      ,sum(query_queue_time_secs) as total_query_queue_time_secs\n",
    "\t  ,avg(actual_execution_time_secs) as avg_actual_execution_time_secs\n",
    "      ,sum(actual_execution_time_secs) as total_actual_execution_time_secs\n",
    "      ,sum(query_cpu_time_secs) as total_query_cpu_time_secs\n",
    "      ,sum(cs_usage_limit_count) as total_cs_usage_limit_count\n",
    "      ,sum(scan_row_count) as total_scan_row_count\n",
    "      ,sum(return_row_count) as total_return_row_count\n",
    "      ,sum(nested_loop_join_row_count) as total_nl_join_row_count\n",
    "  from workload\n",
    "  group by 1,2,3,4,5,6,7\n",
    "  order by 1,2,3,4,5,6,7;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a7213b8-773f-413a-b03d-a88c03538cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries[\"UsagePattern\"] = {}\n",
    "queries[\"UsagePattern\"][\"text\"] = \"\"\"WITH workload_profile AS\n",
    "(\n",
    "  SELECT trim(t.\"database\") AS dbname,\n",
    "         s.statement_type,\n",
    "         CAST(CASE \n",
    "              WHEN regexp_instr (s.statement_text,'(padb_|pg_internal)'              ) THEN 'SYSTEM' \n",
    "              WHEN regexp_instr (s.statement_text,'([uU][nN][dD][oO][iI][nN][gG]) '  ) THEN 'SYSTEM' \n",
    "              WHEN regexp_instr (s.statement_text,'([aA][uU][tT][oO][mM][vV])'       ) THEN 'AUTOMV' \n",
    "              WHEN regexp_instr (s.statement_text,'[uU][nN][lL][oO][aA][dD]'         ) THEN 'UNLOAD' \n",
    "              WHEN regexp_instr (s.statement_text,'[cC][uU][rR][sS][oO][rR] '        ) THEN 'CURSOR' \n",
    "              WHEN regexp_instr (s.statement_text,'[cC][rR][eE][aA][tT][eE] '        ) THEN 'DDL' \n",
    "              WHEN regexp_instr (s.statement_text,'[aA][lL][tT][eE][rR] '            ) THEN 'DDL' \n",
    "              WHEN regexp_instr (s.statement_text,'[dD][rR][oO][pP] '                ) THEN 'DDL' \n",
    "              WHEN regexp_instr (s.statement_text,'[tT][rR][uU][nN][cC][aA][tT][eE] ') THEN 'DDL' \n",
    "              WHEN regexp_instr (s.statement_text,'[cC][aA][lL][lL] '                ) THEN 'CALL_PROCEDURE' \n",
    "              WHEN regexp_instr (s.statement_text,'[gG][rR][aA][nN][tT] '            ) THEN 'DCL' \n",
    "              WHEN regexp_instr (s.statement_text,'[rR][eE][vV][oO][kK][eE] '        ) THEN 'DCL' \n",
    "              WHEN regexp_instr (s.statement_text,'[fF][eE][tT][cC][hH] '            ) THEN 'CURSOR' \n",
    "              WHEN regexp_instr (s.statement_text,'[dD][eE][lL][eE][tT][eE] '        ) THEN 'DELETE'\n",
    "              WHEN regexp_instr (s.statement_text,'[uU][pP][dD][aA][tT][eE] '        ) THEN 'UPDATE' \n",
    "              WHEN regexp_instr (s.statement_text,'[iI][nN][sS][eE][rR][tT] '        ) THEN 'INSERT' \n",
    "              WHEN regexp_instr (s.statement_text,'[vV][aA][cC][uU][uU][mM][ :]'     ) THEN 'VACUUM' \n",
    "              WHEN regexp_instr (s.statement_text,'[aA][nN][aA][lL][yY][zZ][eE] '    ) THEN 'ANALYZE' \n",
    "              WHEN regexp_instr (s.statement_text,'[sS][eE][lL][eE][cC][tT] '        ) THEN 'SELECT' \n",
    "              WHEN regexp_instr (s.statement_text,'[cC][oO][pP][yY] '                ) THEN 'COPY' \n",
    "              ELSE 'OTHER' \n",
    "              END AS VARCHAR(32)) AS query_type,\n",
    "         DATEPART(HOUR,t.starttime) query_hour,\n",
    "\t\t t.starttime::DATE as query_date,\n",
    "         ROUND(SUM(duration_secs),1) query_duration_secs,\n",
    "         COUNT(*) query_count\n",
    "  FROM (SELECT s.userid,\n",
    "               s.type AS statement_type,\n",
    "               s.xid,\n",
    "               s.pid,\n",
    "               DATEDIFF(milliseconds,starttime,endtime)::NUMERIC(38,4) / 1000 AS duration_secs,\n",
    "               CAST(REPLACE(rtrim(s.text),'\\n','') AS VARCHAR(800)) AS statement_text\n",
    "        FROM svl_statementtext s\n",
    "        WHERE s.sequence = 0\n",
    "        AND   s.starttime >= DATEADD(DAY,-7,CURRENT_DATE)\n",
    "       ) s\n",
    "    LEFT OUTER JOIN stl_query t\n",
    "                 ON (s.xid = t.xid\n",
    "                AND s.pid = t.pid\n",
    "                AND s.userid = t.userid)\n",
    "  WHERE s.statement_text NOT LIKE '%volt_tt_%'\n",
    "  GROUP BY 1,2,3,4,5\n",
    ")\n",
    "SELECT wp.dbname,\n",
    "       wp.query_date,\n",
    "       wp.query_hour,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'SELECT' THEN query_count ELSE 0 END,0))::bigint AS select_count,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'SELECT' THEN query_duration_secs ELSE 0 END,0)) AS select_duration_secs,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'CURSOR' THEN query_count ELSE 0 END,0))::bigint AS cursor_count,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'CURSOR' THEN query_duration_secs ELSE 0 END,0)) AS cursor_duration_secs,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'UNLOAD' THEN query_count ELSE 0 END,0))::bigint AS unload_count,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'UNLOAD' THEN query_duration_secs ELSE 0 END,0)) AS unload_duration_secs,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'COPY' THEN query_count ELSE 0 END,0))::bigint AS copy_count,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'COPY' THEN query_duration_secs ELSE 0 END,0)) AS copy_duration_secs,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'INSERT' THEN query_count ELSE 0 END,0))::bigint AS insert_count,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'INSERT' THEN query_duration_secs ELSE 0 END,0)) AS insert_duration_secs,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'UPDATE' THEN query_count ELSE 0 END,0))::bigint AS update_count,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'UPDATE' THEN query_duration_secs ELSE 0 END,0)) AS update_duration_secs,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'DELETE' THEN query_count ELSE 0 END,0))::bigint AS delete_count,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'DELETE' THEN query_duration_secs ELSE 0 END,0)) AS delete_duration_secs,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'VACUUM' THEN query_count ELSE 0 END,0))::bigint AS vacuum_count,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'VACUUM' THEN query_duration_secs ELSE 0 END,0)) AS vacuum_duration_secs,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'ANALYZE' THEN query_count ELSE 0 END,0))::bigint AS analyze_count,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'ANALYZE' THEN query_duration_secs ELSE 0 END,0)) AS analyze_duration_secs,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'CALL_PROCEDURE' THEN query_count ELSE 0 END,0))::bigint AS call_proc_count,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'CALL_PROCEDURE' THEN query_duration_secs ELSE 0 END,0)) AS call_proc_duration_secs,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'DDL' THEN query_count ELSE 0 END,0))::bigint AS ddl_count,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'DDL' THEN query_duration_secs ELSE 0 END,0)) AS ddl_duration_secs,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'DCL' THEN query_count ELSE 0 END,0))::bigint AS dcl_count,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'DCL' THEN query_duration_secs ELSE 0 END,0)) AS dcl_duration_secs,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'AUTOMV' THEN query_count ELSE 0 END,0))::bigint AS automv_count,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'AUTOMV' THEN query_duration_secs ELSE 0 END,0)) AS automv_duration_secs,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'OTHER' THEN query_count ELSE 0 END,0))::bigint AS other_count,\n",
    "       MAX(NVL(CASE WHEN wp.query_type = 'OTHER' THEN query_duration_secs ELSE 0 END,0)) AS other_duration_secs\n",
    "FROM workload_profile wp\n",
    "WHERE wp.dbname <> 'padb_harvest'\n",
    "GROUP BY 1,2,3\n",
    "ORDER BY 1,2,3;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1569a099-94b3-4f15-a5b9-130ada59b022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries[\"ConcurrencyScalingUsage\"] = {}\n",
    "queries[\"ConcurrencyScalingUsage\"][\"text\"] = \"\"\"select date_trunc('hour',end_time) as burst_hour\n",
    "      ,sum(queries) as query_count\n",
    "      ,sum(usage_in_seconds) as burst_usage_in_seconds  \n",
    "from svcs_concurrency_scaling_usage\n",
    "group by burst_hour;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9c1d178-8573-4c2c-8138-7f9ca7dd4bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries[\"TableAlerts\"] = {}\n",
    "queries[\"TableAlerts\"][\"text\"] = \"\"\"SELECT trim(q.\"database\") as dbname,\n",
    "       trim(s.perm_table_name) AS table_name,\n",
    "       COALESCE(SUM(ABS(datediff (microsecond,COALESCE(b.starttime,d.starttime,s.starttime),CASE WHEN COALESCE(b.endtime,d.endtime,s.endtime) >COALESCE(b.starttime,d.starttime,s.starttime) THEN COALESCE(b.endtime,d.endtime,s.endtime) ELSE COALESCE(b.starttime,d.starttime,s.starttime) END)))/ 1000000::NUMERIC(38,4),0) AS alert_seconds,\n",
    "       COALESCE(SUM(COALESCE(b.rows,d.rows,s.rows)),0) AS alert_rowcount,\n",
    "       trim(split_part (l.event,':',1)) AS alert_event,\n",
    "       substring(trim(l.solution),1,200) AS alert_solution,\n",
    "       MAX(l.query) AS alert_sample_query,\n",
    "       COUNT(DISTINCT l.query) alert_querycount\n",
    "FROM stl_alert_event_log AS l\n",
    "  LEFT JOIN stl_scan AS s\n",
    "         ON s.query = l.query\n",
    "        AND s.slice = l.slice\n",
    "        AND s.segment = l.segment\n",
    "        AND s.userid > 1\n",
    "        AND s.perm_table_name NOT IN ('Internal Worktable','S3')\n",
    "\t\tAND s.perm_table_name NOT LIKE ('volt_tt%')\n",
    "\t\tAND s.perm_table_name NOT LIKE ('mv_tbl__auto_mv%')\t\t\t\n",
    "  LEFT JOIN stl_dist AS d\n",
    "         ON d.query = l.query\n",
    "        AND d.slice = l.slice\n",
    "        AND d.segment = l.segment\n",
    "        AND d.userid > 1\n",
    "  LEFT JOIN stl_bcast AS b\n",
    "         ON b.query = l.query\n",
    "        AND b.slice = l.slice\n",
    "        AND b.segment = l.segment\n",
    "        AND b.userid > 1    \n",
    "  LEFT JOIN stl_query AS q   \n",
    "         ON q.query = l.query\n",
    "        AND q.xid = l.xid\n",
    "        AND q.userid > 1           \n",
    "WHERE l.userid > 1\n",
    "  AND trim(s.perm_table_name) IS NOT NULL\n",
    "  AND l.event_time >= dateadd(day,- 7,CURRENT_DATE)\n",
    "GROUP BY 1,2,5,6\n",
    "ORDER BY alert_seconds DESC;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd14041e-c7fc-4842-9b31-891ccef8cc69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries[\"TableInfo\"] = {}\n",
    "queries[\"TableInfo\"][\"text\"] =\"\"\"SELECT t.\"database\" AS dbname,\n",
    "       t.\"schema\" AS namespace,\n",
    "       t.\"table\" table_name,\n",
    "       t.encoded,\n",
    "       t.diststyle,\n",
    "       t.sortkey1,\n",
    "       t.max_varchar,\n",
    "       trim(t.sortkey1_enc) AS sortkey1_enc,\n",
    "       t.sortkey_num,\n",
    "       t.unsorted,\n",
    "       t.stats_off,\n",
    "       t.tbl_rows,\n",
    "       t.skew_rows,\n",
    "       t.estimated_visible_rows,\n",
    "       CASE\n",
    "         WHEN t.tbl_rows - t.estimated_visible_rows < 0 THEN 0\n",
    "         ELSE (t.tbl_rows - t.estimated_visible_rows)\n",
    "       END AS num_rows_marked_for_deletion,\n",
    "       CASE\n",
    "         WHEN t.tbl_rows - t.estimated_visible_rows < 0 THEN 0\n",
    "         ELSE (t.tbl_rows - t.estimated_visible_rows) /\n",
    "           CASE\n",
    "             WHEN nvl (t.tbl_rows,0) = 0 THEN 1\n",
    "             ELSE t.tbl_rows\n",
    "           END ::NUMERIC(38,4)\n",
    "       END AS pct_rows_marked_for_deletion,\n",
    "       t.vacuum_sort_benefit,\n",
    "       v.vacuum_run_type,\n",
    "       v.is_last_vacuum_recluster,\n",
    "       v.last_vacuumed_date,\n",
    "       v.days_since_last_vacuumed,\n",
    "       NVL(s.num_qs,0) query_count,\n",
    "       nvl(sat.table_recommendation_count,0) AS table_recommendation_count,\n",
    "       c.encoded_column_count,\n",
    "       c.column_count,\n",
    "       c.encoded_column_pct::NUMERIC(38,4) AS encoded_column_pct,\n",
    "       c.encoded_sortkey_count,\n",
    "       c.distkey_column_count,\n",
    "       nvl(tc.large_column_size_count,0) AS large_column_size_count,\n",
    "       tak.alert_sample_query AS sort_key_alert_sample_query,\n",
    "       nvl(tak.alert_query_count,0) AS sort_key_alert_query_count,\n",
    "       tas.alert_sample_query AS stats_alert_sample_query,\n",
    "       nvl(tas.alert_query_count,0) AS stats_alert_query_count,\n",
    "       tanl.alert_sample_query AS nl_alert_sample_query,\n",
    "       nvl(tanl.alert_query_count,0) AS nl_alert_query_count,\n",
    "       tad.alert_sample_query AS distributed_alert_sample_query,\n",
    "       nvl(tad.alert_query_count,0) AS distributed_alert_query_count,\n",
    "       tab.alert_sample_query AS distributed_alert_sample_query,\n",
    "       nvl(tab.alert_query_count,0) AS broadcasted_alert_query_count,\n",
    "       tax.alert_sample_query AS deleted_alert_sample_query,\n",
    "       nvl(tax.alert_query_count,0) AS deleted_alert_query_count\n",
    "FROM SVV_TABLE_INFO t\n",
    "  INNER JOIN (SELECT attrelid,\n",
    "                     COUNT(1) column_count,\n",
    "                     SUM(CASE WHEN attisdistkey = FALSE THEN 0 ELSE 1 END) AS distkey_column_count,\n",
    "                     SUM(CASE WHEN attencodingtype IN (0,128) THEN 0 ELSE 1 END) AS encoded_column_count,\n",
    "                     1.0 *SUM(CASE WHEN attencodingtype IN (0,128) THEN 0 ELSE 1 END) / COUNT(1)*100 encoded_column_pct,\n",
    "                     SUM(CASE WHEN attencodingtype NOT IN (0,128) AND attsortkeyord > 0 THEN 1 ELSE 0 END) AS encoded_sortkey_count\n",
    "              FROM pg_attribute\n",
    "              WHERE attnum > 0\n",
    "              GROUP BY attrelid) c ON (c.attrelid = t.table_id)\n",
    "  LEFT OUTER JOIN (SELECT tbl,\n",
    "                          perm_table_name,\n",
    "                          COUNT(DISTINCT query) num_qs\n",
    "                   FROM stl_scan s\n",
    "                   WHERE s.userid > 1\n",
    "                   AND   s.perm_table_name NOT IN ('Internal Worktable','S3')\n",
    "                   GROUP BY 1,\n",
    "                            2) s ON (s.tbl = t.table_id)\n",
    "  LEFT OUTER JOIN (SELECT DATABASE,\n",
    "                          table_id,\n",
    "                          COUNT(1) AS table_recommendation_count\n",
    "                   FROM svv_alter_table_recommendations\n",
    "                   GROUP BY DATABASE,\n",
    "                            table_id) sat\n",
    "               ON (sat.table_id = t.table_id\n",
    "              AND sat.database = t.database)\n",
    "  LEFT OUTER JOIN (SELECT sc.table_catalog AS database_name,\n",
    "                          sc.table_schema,\n",
    "                          sc.table_name,\n",
    "                          SUM(CASE WHEN sc.character_maximum_length > 1000 THEN 1 ELSE 0 END) AS large_column_size_count\n",
    "                   FROM svv_columns sc\n",
    "                     INNER JOIN svv_table_info st\n",
    "                             ON (sc.table_catalog = st.database\n",
    "                            AND sc.table_schema = st.schema\n",
    "                            AND sc.table_name = st.table)\n",
    "                   WHERE sc.data_type IN ('character varying','character')\n",
    "                   AND   sc.character_maximum_length > 1000\n",
    "                   AND   sc.table_schema NOT IN ('pg_internal','pg_catalog','pg_automv')\n",
    "                   GROUP BY sc.table_catalog,\n",
    "                            sc.table_schema,\n",
    "                            sc.table_name) tc\n",
    "               ON (t.database = tc.database_name\n",
    "              AND tc.table_name = t.table\n",
    "              AND tc.table_schema = t.schema)\n",
    "  LEFT OUTER JOIN (SELECT t.database AS database_name,\n",
    "                          t.schema AS schema_name,\n",
    "                          t.table AS table_name,\n",
    "                          v.table_id,\n",
    "                          CASE\n",
    "                            WHEN v.status LIKE '%VacuumBG%' THEN 'Automatic'\n",
    "                            ELSE 'Manual'\n",
    "                          END AS vacuum_run_type,\n",
    "                          CASE\n",
    "                            WHEN v.is_recluster = 0 THEN 'N'\n",
    "                            WHEN v.is_recluster = 1 THEN 'Y'\n",
    "                            ELSE NULL\n",
    "                          END AS is_last_vacuum_recluster,\n",
    "                          CAST(MAX(v.eventtime) AS DATE) AS last_vacuumed_date,\n",
    "                          datediff(d,CAST(MAX(v.eventtime) AS DATE),CAST(CURRENT_TIMESTAMP AT TIME ZONE 'UTC' AS DATE)) AS days_since_last_vacuumed\n",
    "                   FROM stl_vacuum v\n",
    "                     INNER JOIN svv_table_info t ON (t.table_id = v.table_id)\n",
    "                   WHERE v.status NOT LIKE '%Started%'\n",
    "                   GROUP BY 1,\n",
    "                            2,\n",
    "                            3,\n",
    "                            4,\n",
    "                            5,\n",
    "                            6) v\n",
    "               ON (v.database_name = t.database\n",
    "              AND v.table_name = t.table\n",
    "              AND v.schema_name = t.schema)\n",
    "  LEFT OUTER JOIN (SELECT q.database AS database_name,\n",
    "                          TRIM(s.perm_table_name) AS table_name,\n",
    "                          MAX(l.query) AS alert_sample_query,\n",
    "                          COUNT(DISTINCT l.query) AS alert_query_count\n",
    "                   FROM stl_alert_event_log AS l\n",
    "                     LEFT JOIN stl_scan AS s\n",
    "                            ON s.query = l.query\n",
    "                           AND s.slice = l.slice\n",
    "                           AND s.segment = l.segment\n",
    "                           AND s.userid > 1\n",
    "                           AND s.perm_table_name NOT IN ('Internal Worktable', 'S3') \n",
    "                     LEFT JOIN stl_query AS q\n",
    "                            ON q.query = l.query\n",
    "                           AND q.xid = l.xid\n",
    "                           AND q.userid > 1\n",
    "                   WHERE l.userid > 1\n",
    "                   AND   TRIM(s.perm_table_name) IS NOT NULL\n",
    "                   AND   l.event_time >= dateadd(DAY,- 7,CURRENT_DATE)\n",
    "                   AND   TRIM(split_part(l.event,':',1)) = 'Very selective query filter'\n",
    "                   GROUP BY 1,\n",
    "                            2) tak\n",
    "               ON (tak.database_name = t.database\n",
    "              AND tak.table_name = t.table)\n",
    "  LEFT OUTER JOIN (SELECT q.database AS database_name,\n",
    "                          TRIM(s.perm_table_name) AS table_name,\n",
    "                          MAX(l.query) AS alert_sample_query,\n",
    "                          COUNT(DISTINCT l.query) AS alert_query_count\n",
    "                   FROM stl_alert_event_log AS l\n",
    "                     LEFT JOIN stl_scan AS s\n",
    "                            ON s.query = l.query\n",
    "                           AND s.slice = l.slice\n",
    "                           AND s.segment = l.segment\n",
    "                           AND s.userid > 1\n",
    "                           AND s.perm_table_name NOT IN ('Internal Worktable', 'S3') \n",
    "                     LEFT JOIN stl_query AS q\n",
    "                            ON q.query = l.query\n",
    "                           AND q.xid = l.xid\n",
    "                           AND q.userid > 1\n",
    "                   WHERE l.userid > 1\n",
    "                   AND   TRIM(s.perm_table_name) IS NOT NULL\n",
    "                   AND   l.event_time >= dateadd(DAY,- 7,CURRENT_DATE)\n",
    "                   AND   TRIM(split_part(l.event,':',1)) = 'Missing query planner statistics'\n",
    "                   GROUP BY 1,\n",
    "                            2) tas\n",
    "               ON (tas.database_name = t.database\n",
    "              AND tas.table_name = t.table)\n",
    "  LEFT OUTER JOIN (SELECT q.database AS database_name,\n",
    "                          TRIM(s.perm_table_name) AS table_name,\n",
    "                          MAX(l.query) AS alert_sample_query,\n",
    "                          COUNT(DISTINCT l.query) AS alert_query_count\n",
    "                   FROM stl_alert_event_log AS l\n",
    "                     LEFT JOIN stl_scan AS s\n",
    "                            ON s.query = l.query\n",
    "                           AND s.slice = l.slice\n",
    "                           AND s.segment = l.segment\n",
    "                           AND s.userid > 1\n",
    "                           AND s.perm_table_name NOT IN ('Internal Worktable', 'S3') \n",
    "                     LEFT JOIN stl_query AS q\n",
    "                            ON q.query = l.query\n",
    "                           AND q.xid = l.xid\n",
    "                           AND q.userid > 1\n",
    "                   WHERE l.userid > 1\n",
    "                   AND   TRIM(s.perm_table_name) IS NOT NULL\n",
    "                   AND   l.event_time >= dateadd(DAY,- 7,CURRENT_DATE)\n",
    "                   AND   TRIM(split_part(l.event,':',1)) = 'Nested Loop Join in the query plan'\n",
    "                   GROUP BY 1,\n",
    "                            2) tanl\n",
    "               ON (tanl.database_name = t.database\n",
    "              AND tanl.table_name = t.table)\n",
    "  LEFT OUTER JOIN (SELECT q.database AS database_name,\n",
    "                          TRIM(s.perm_table_name) AS table_name,\n",
    "                          MAX(l.query) AS alert_sample_query,\n",
    "                          COUNT(DISTINCT l.query) AS alert_query_count\n",
    "                   FROM stl_alert_event_log AS l\n",
    "                     LEFT JOIN stl_scan AS s\n",
    "                            ON s.query = l.query\n",
    "                           AND s.slice = l.slice\n",
    "                           AND s.segment = l.segment\n",
    "                           AND s.userid > 1\n",
    "                           AND s.perm_table_name NOT IN ('Internal Worktable', 'S3') \n",
    "                     LEFT JOIN stl_query AS q\n",
    "                            ON q.query = l.query\n",
    "                           AND q.xid = l.xid\n",
    "                           AND q.userid > 1\n",
    "                   WHERE l.userid > 1\n",
    "                   AND   TRIM(s.perm_table_name) IS NOT NULL\n",
    "                   AND   l.event_time >= dateadd(DAY,- 7,CURRENT_DATE)\n",
    "                   AND   TRIM(split_part(l.event,':',1)) = 'Distributed a large number of rows across the network'\n",
    "                   GROUP BY 1,\n",
    "                            2) tad\n",
    "               ON (tad.database_name = t.database\n",
    "              AND tad.table_name = t.table)\n",
    "  LEFT OUTER JOIN (SELECT q.database AS database_name,\n",
    "                          TRIM(s.perm_table_name) AS table_name,\n",
    "                          MAX(l.query) AS alert_sample_query,\n",
    "                          COUNT(DISTINCT l.query) AS alert_query_count\n",
    "                   FROM stl_alert_event_log AS l\n",
    "                     LEFT JOIN stl_scan AS s\n",
    "                            ON s.query = l.query\n",
    "                           AND s.slice = l.slice\n",
    "                           AND s.segment = l.segment\n",
    "                           AND s.userid > 1\n",
    "                           AND s.perm_table_name NOT IN ('Internal Worktable', 'S3') \n",
    "                     LEFT JOIN stl_query AS q\n",
    "                            ON q.query = l.query\n",
    "                           AND q.xid = l.xid\n",
    "                           AND q.userid > 1\n",
    "                   WHERE l.userid > 1\n",
    "                   AND   TRIM(s.perm_table_name) IS NOT NULL\n",
    "                   AND   l.event_time >= dateadd(DAY,- 7,CURRENT_DATE)\n",
    "                   AND   TRIM(split_part(l.event,':',1)) = 'Broadcasted a large number of rows across the network'\n",
    "                   GROUP BY 1,\n",
    "                            2) tab\n",
    "               ON (tab.database_name = t.database\n",
    "              AND tab.table_name = t.table)\n",
    "  LEFT OUTER JOIN (SELECT q.database AS database_name,\n",
    "                          TRIM(s.perm_table_name) AS table_name,\n",
    "                          MAX(l.query) AS alert_sample_query,\n",
    "                          COUNT(DISTINCT l.query) AS alert_query_count\n",
    "                   FROM stl_alert_event_log AS l\n",
    "                     LEFT JOIN stl_scan AS s\n",
    "                            ON s.query = l.query\n",
    "                           AND s.slice = l.slice\n",
    "                           AND s.segment = l.segment\n",
    "                           AND s.userid > 1\n",
    "                           AND s.perm_table_name NOT IN ('Internal Worktable', 'S3') \n",
    "                     LEFT JOIN stl_query AS q\n",
    "                            ON q.query = l.query\n",
    "                           AND q.xid = l.xid\n",
    "                           AND q.userid > 1\n",
    "                   WHERE l.userid > 1\n",
    "                   AND   TRIM(s.perm_table_name) IS NOT NULL\n",
    "                   AND   l.event_time >= dateadd(DAY,- 7,CURRENT_DATE)\n",
    "                   AND   TRIM(split_part(l.event,':',1)) = 'Scanned a large number of deleted rows'\n",
    "                   GROUP BY 1,\n",
    "                            2) tax\n",
    "               ON (tax.database_name = t.database\n",
    "              AND tax.table_name = t.table)\n",
    "WHERE t.\"schema\" NOT IN ('pg_internal','pg_catalog','pg_automv')\n",
    "AND   t.\"schema\" NOT LIKE 'pg_temp%'\n",
    "ORDER BY tbl_rows DESC;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50b7e1ff-24af-49e8-939f-4f00f2e3faaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries[\"AlterTableRecommendation\"]=\"\"\"SELECT r.type,\n",
    "       trim(t.database_name) AS dbname,\n",
    "       t.schema_name AS namespace,\n",
    "       r.table_id,\n",
    "       t.table_name,\n",
    "       r.group_id,\n",
    "       r.ddl,\n",
    "       r.auto_eligible\n",
    "FROM svv_alter_table_recommendations r\n",
    "  INNER JOIN (SELECT DISTINCT(stv_tbl_perm.id) AS table_id\n",
    "                    ,TRIM(pg_database.datname) AS database_name\n",
    "                    ,TRIM(pg_namespace.nspname) AS schema_name\n",
    "                    ,TRIM(relname) AS table_name\n",
    "                FROM stv_tbl_perm\n",
    "              INNER JOIN pg_database on pg_database.oid = stv_tbl_perm.db_id\n",
    "              INNER JOIN pg_class on pg_class.oid = stv_tbl_perm.id\n",
    "              INNER JOIN pg_namespace on pg_namespace.oid = pg_class.relnamespace\n",
    "               WHERE schema_name NOT IN ('pg_internal', 'pg_catalog','pg_automv')) t\n",
    "          ON (r.database = t.database_name\n",
    "         AND r.table_id = t.table_id);\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03b5855f-b38f-46c3-a8fd-e896197d7059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries[\"MaterializedView\"]=\"\"\"WITH mv_info AS\n",
    "(\n",
    "  SELECT trim(db_name) AS dbname,\n",
    "         trim(\"schema\") AS namespace, \n",
    "         trim(name) AS mview_name,\n",
    "         is_stale,\n",
    "         state,\n",
    "         CASE state\n",
    "           WHEN 0 THEN 'The MV is fully recomputed when refreshed'\n",
    "           WHEN 1 THEN 'The MV is incremental'\n",
    "           WHEN 101 THEN 'The MV cant be refreshed due to a dropped column. This constraint applies even if the column isnt used in the MV'\n",
    "           WHEN 102 THEN 'The MV cant be refreshed due to a changed column type. This constraint applies even if the column isnt used in the MV'\n",
    "           WHEN 103 THEN 'The MV cant be refreshed due to a renamed table'\n",
    "           WHEN 104 THEN 'The MV cant be refreshed due to a renamed column. This constraint applies even if the column isnt used in the MV'\n",
    "           WHEN 105 THEN 'The MV cant be refreshed due to a renamed schema'\n",
    "           ELSE NULL\n",
    "         END AS state_desc,\n",
    "         autorewrite,\n",
    "         autorefresh\n",
    "  FROM stv_mv_info\n",
    "),\n",
    "mv_state AS\n",
    "(\n",
    "  SELECT dbname,\n",
    "         namespace,\n",
    "         mview_name,\n",
    "         state AS mv_state,\n",
    "         event_desc,\n",
    "         starttime AS event_starttime\n",
    "  FROM (SELECT trim(db_name) AS dbname,\n",
    "               trim(mv_schema) AS namespace, \n",
    "               trim(mv_name) AS mview_name,\n",
    "               ROW_NUMBER() OVER (PARTITION BY db_name, mv_schema, mv_name ORDER BY starttime DESC) AS rnum,\n",
    "               state,\n",
    "               event_desc,\n",
    "               starttime\n",
    "        FROM stl_mv_state)\n",
    "  WHERE rnum = 1\n",
    "),\n",
    "mv_ref_status AS\n",
    "(\n",
    "  SELECT dbname,\n",
    "         refresh_db_username,\n",
    "         namespace,\n",
    "         mview_name,\n",
    "         status AS refresh_status,\n",
    "         refresh_type,\n",
    "         starttime AS refresh_starttime,\n",
    "         endtime AS refresh_endtime,\n",
    "         datediff(ms, starttime, endtime)*1.0/1000 AS refresh_duration_secs\n",
    "  FROM (SELECT trim(r.db_name) AS dbname,\n",
    "               trim(r.schema_name) AS namespace, \n",
    "               pu.usename as refresh_db_username,\n",
    "               trim(r.mv_name) AS mview_name,\n",
    "               ROW_NUMBER() OVER (PARTITION BY r.db_name, r.schema_name, r.userid, r.mv_name ORDER BY starttime DESC) AS rnum,\n",
    "               r.status,\n",
    "               r.refresh_type,\n",
    "               r.starttime,\n",
    "               r.endtime\n",
    "        FROM svl_mv_refresh_status r\n",
    "        INNER JOIN pg_user pu on (r.userid = pu.usesysid))\n",
    "  WHERE rnum = 1\n",
    ")\n",
    "SELECT i.*, \n",
    "       s.mv_state, s.event_desc, s.event_starttime,\n",
    "       r.refresh_db_username, r.refresh_status, r.refresh_type, r.refresh_starttime, r.refresh_endtime, r.refresh_duration_secs\n",
    "FROM mv_info i\n",
    "  LEFT JOIN mv_state s ON (i.dbname = s.dbname AND i.namespace = s.namespace AND i.mview_name = s.mview_name)\n",
    "  LEFT JOIN mv_ref_status r ON (i.dbname = r.dbname AND i.namespace = r.namespace AND i.mview_name = r.mview_name);\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78c61cc9-f52a-4be8-b9aa-521723d46095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries[\"Top50QueriesByRunTime\"]=\"\"\"SELECT TRIM(dbname) AS dbname,\n",
    "       TRIM(db_username) AS db_username,\n",
    "       MAX(SUBSTRING(replace(qrytext,chr (34),chr (92) + chr (34)),1,500)) AS qrytext,\n",
    "       COUNT(query) AS num_queries,\n",
    "       MIN(run_minutes) AS min_minutes,\n",
    "       MAX(run_minutes) AS max_minutes,\n",
    "       AVG(run_minutes) AS avg_minutes,\n",
    "       SUM(run_minutes) AS total_minutes,\n",
    "       SUM(compile_minutes) AS total_compile_minutes,\n",
    "       SUM(num_compile_segments) AS total_num_compile_segments,\n",
    "       MIN(query_temp_blocks_to_disk_mb) AS min_disk_spill_mb,\n",
    "       MAX(query_temp_blocks_to_disk_mb) AS max_disk_spill_mb,\n",
    "       AVG(query_temp_blocks_to_disk_mb) AS avg_disk_spill_mb,\n",
    "       SUM(query_temp_blocks_to_disk_mb) AS total_disk_spill_mb,\n",
    "       MAX(query) AS max_query_id,\n",
    "       MAX(starttime)::DATE AS last_run,\n",
    "       COUNT(DISTINCT starttime::DATE) AS num_days_executed,\n",
    "       SUM(aborted) AS total_aborted,\n",
    "       MAX(mylabel) qry_label,\n",
    "       AVG(spectrum_object_count) AS avg_spectrum_object_used,\n",
    "       AVG(federated_object_count) AS avg_federated_object_used,\n",
    "       user_table_involved,\n",
    "       TRIM(DECODE (event & 1,1,'Sortkey ','') || DECODE (event & 2,2,'Deletes ','') || DECODE (event & 4,4,'NL ','') || DECODE (event & 8,8,'Dist ','') || DECODE (event & 16,16,'Broacast ','') || DECODE (event & 32,32,'Stats ','')) AS Alert\n",
    "FROM (SELECT stl_query.userid,\n",
    "             pu.usename as db_username,\n",
    "             label,\n",
    "             stl_query.query,\n",
    "             TRIM(\"DATABASE\") AS dbname,\n",
    "             NVL(qrytext_cur.text,TRIM(querytxt)) AS qrytext,\n",
    "             MD5(NVL (qrytext_cur.text,TRIM(querytxt))) AS qry_md5,\n",
    "             starttime,\n",
    "             endtime,\n",
    "             DATEDIFF(seconds,starttime,endtime)::NUMERIC(12,2) / 60 AS run_minutes,\n",
    "             aborted,\n",
    "             event,\n",
    "             stl_query.label AS mylabel,\n",
    "             CASE\n",
    "               WHEN sqms.query_temp_blocks_to_disk IS NULL THEN 0\n",
    "               ELSE sqms.query_temp_blocks_to_disk\n",
    "             END AS query_temp_blocks_to_disk_mb,\n",
    "             nvl(compile_secs,0)::NUMERIC(12,2) / 60 AS compile_minutes,\n",
    "             nvl(num_compile_segments,0) AS num_compile_segments,\n",
    "             s.user_table_involved,\n",
    "             NVL(s3.spectrum_object_count,0) AS spectrum_object_count,\n",
    "             NVL(f.federated_object_count,0) AS federated_object_count\n",
    "      FROM stl_query\n",
    "\t  INNER JOIN pg_catalog.pg_user pu on (stl_query.userid = pu.usesysid)\n",
    "        LEFT OUTER JOIN (SELECT query,\n",
    "                                SUM(DECODE (TRIM(SPLIT_PART (event,':',1)),'Very selective query filter',1,'Scanned a large number of deleted rows',2,'Nested Loop Join in the query plan',4,'Distributed a large number of rows across the network',8,'Broadcasted a large number of rows across the network',16,'Missing query planner statistics',32,0)) AS event\n",
    "                         FROM stl_alert_event_log\n",
    "                         WHERE event_time >= DATEADD(DAY,-7,CURRENT_DATE)\n",
    "                         GROUP BY query) AS alrt ON alrt.query = stl_query.query\n",
    "        LEFT OUTER JOIN (SELECT ut.xid,\n",
    "                                TRIM(SUBSTRING(text FROM STRPOS (UPPER(text),'SELECT'))) AS TEXT\n",
    "                         FROM stl_utilitytext ut\n",
    "                         WHERE SEQUENCE = 0\n",
    "                         AND   text ilike 'DECLARE%'\n",
    "                         GROUP BY text,\n",
    "                                  ut.xid) qrytext_cur ON (stl_query.xid = qrytext_cur.xid)\n",
    "        LEFT OUTER JOIN svl_query_metrics_summary sqms\n",
    "                     ON (sqms.userid = stl_query.userid\n",
    "                    AND sqms.query = stl_query.query)\n",
    "        LEFT OUTER JOIN (SELECT userid,\n",
    "                                xid,\n",
    "                                pid,\n",
    "                                query,\n",
    "                                MAX(datediff (ms,starttime,endtime)*1.0 / 1000) AS compile_secs,\n",
    "                                SUM(compile) AS num_compile_segments\n",
    "                         FROM svcs_compile\n",
    "                         GROUP BY userid,\n",
    "                                  xid,\n",
    "                                  pid,\n",
    "                                  query) c\n",
    "                     ON (c.userid = stl_query.userid\n",
    "                    AND c.xid = stl_query.xid\n",
    "                    AND c.pid = stl_query.pid\n",
    "                    AND c.query = stl_query.query)\n",
    "        LEFT OUTER JOIN (SELECT s.userid,\n",
    "                                s.query,\n",
    "                                LISTAGG(DISTINCT TRIM(s.perm_table_name),', ') AS user_table_involved\n",
    "                         FROM stl_scan s\n",
    "                             INNER JOIN (SELECT DISTINCT(stv_tbl_perm.id) AS table_id\n",
    "                    ,TRIM(pg_database.datname) AS database_name\n",
    "                    ,TRIM(pg_namespace.nspname) AS schema_name\n",
    "                    ,TRIM(relname) AS table_name\n",
    "                FROM stv_tbl_perm\n",
    "              INNER JOIN pg_database on pg_database.oid = stv_tbl_perm.db_id\n",
    "              INNER JOIN pg_class on pg_class.oid = stv_tbl_perm.id\n",
    "              INNER JOIN pg_namespace on pg_namespace.oid = pg_class.relnamespace\n",
    "               WHERE schema_name NOT IN ('pg_internal', 'pg_catalog','pg_automv')) t ON (s.tbl = t.table_id)\n",
    "                         WHERE s.perm_table_name NOT IN ('Internal Worktable','S3')\n",
    "                         AND   s.perm_table_name NOT LIKE ('volt_tt%')\n",
    "                         AND   t.schema_name NOT IN ('pg_internal','pg_catalog')\n",
    "                         GROUP BY s.userid,\n",
    "                                  s.query) s\n",
    "                     ON (stl_query.userid = s.userid\n",
    "                    AND stl_query.query = s.query)\n",
    "        LEFT OUTER JOIN (SELECT s.userid,\n",
    "                                s.query,\n",
    "                                COUNT(1) AS spectrum_object_count\n",
    "                         FROM svl_s3query_summary s\n",
    "                         WHERE s.external_table_name NOT IN ('PG Subquery')\n",
    "                         GROUP BY s.userid,\n",
    "                                  s.query) s3\n",
    "                     ON (stl_query.userid = s3.userid\n",
    "                    AND stl_query.query = s3.query)\n",
    "        LEFT OUTER JOIN (SELECT f.userid,\n",
    "                                f.query,\n",
    "                                COUNT(1) AS federated_object_count\n",
    "                         FROM svl_federated_query f\n",
    "                         GROUP BY f.userid,\n",
    "                                  f.query) f\n",
    "                     ON (stl_query.userid = f.userid\n",
    "                    AND stl_query.query = f.query)\n",
    "      WHERE stl_query.userid <> 1\n",
    "      AND   NVL(qrytext_cur.text,TRIM(querytxt)) NOT LIKE 'padb_fetch_sample:%'\n",
    "      AND   NVL(qrytext_cur.text,TRIM(querytxt)) NOT LIKE 'CREATE TEMP TABLE volt_tt_%'\n",
    "      AND   stl_query.starttime >= DATEADD(DAY,-7,CURRENT_DATE))\n",
    "GROUP BY TRIM(dbname),\n",
    "         TRIM(db_username),\n",
    "         qry_md5,\n",
    "         user_table_involved,\n",
    "         event\n",
    "ORDER BY avg_minutes DESC, num_queries DESC LIMIT 50;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d73e2a9-f73f-4a28-b384-fb962f9452e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries[\"Top50QueriesByDiskSpill\"]=\"\"\"SELECT TRIM(dbname) AS dbname,\n",
    "       TRIM(db_username) AS db_username,\n",
    "       MAX(SUBSTRING(replace(qrytext,chr (34),chr (92) + chr (34)),1,500)) AS qrytext,\n",
    "       COUNT(query) AS num_queries,\t    \n",
    "       MIN(run_minutes) AS min_minutes,\n",
    "       MAX(run_minutes) AS max_minutes,\n",
    "       AVG(run_minutes) AS avg_minutes,\n",
    "       SUM(run_minutes) AS total_minutes,\n",
    "       SUM(compile_minutes) AS total_compile_minutes,\n",
    "       SUM(num_compile_segments) AS total_num_compile_segments,\n",
    "       MIN(query_temp_blocks_to_disk_mb) AS min_disk_spill_mb,\n",
    "       MAX(query_temp_blocks_to_disk_mb) AS max_disk_spill_mb,\n",
    "       AVG(query_temp_blocks_to_disk_mb) AS avg_disk_spill_mb,\n",
    "       SUM(query_temp_blocks_to_disk_mb) AS total_disk_spill_mb,\n",
    "       MAX(query) AS max_query_id,\n",
    "       MAX(starttime)::DATE AS last_run,\n",
    "       COUNT(DISTINCT starttime::DATE) AS num_days_executed,\t\t   \n",
    "       SUM(aborted) as total_aborted,\n",
    "       MAX(mylabel) qry_label,\n",
    "\t   AVG(spectrum_object_count) AS avg_spectrum_object_used,\n",
    "\t   AVG(federated_object_count) AS avg_federated_object_used,\n",
    "\t   user_table_involved,\n",
    "       TRIM(DECODE (event & 1,1,'Sortkey ','') || DECODE (event & 2,2,'Deletes ','') || DECODE (event & 4,4,'NL ','') || DECODE (event & 8,8,'Dist ','') || DECODE (event & 16,16,'Broacast ','') || DECODE (event & 32,32,'Stats ','')) AS Alert\n",
    "FROM (SELECT stl_query.userid,\n",
    "             pu.usename as db_username,\n",
    "             label,\n",
    "             stl_query.query,\n",
    "             TRIM(\"DATABASE\") AS dbname,\n",
    "             NVL(qrytext_cur.text,TRIM(querytxt)) AS qrytext,\n",
    "             MD5(NVL (qrytext_cur.text,TRIM(querytxt))) AS qry_md5,\n",
    "             starttime,\n",
    "             endtime,\n",
    "             DATEDIFF(seconds,starttime,endtime)::NUMERIC(12,2) / 60 AS run_minutes,\n",
    "             aborted,\n",
    "             event,\n",
    "             stl_query.label AS mylabel,\n",
    "             CASE\n",
    "               WHEN sqms.query_temp_blocks_to_disk IS NULL THEN 0\n",
    "               ELSE sqms.query_temp_blocks_to_disk\n",
    "             END AS query_temp_blocks_to_disk_mb,\n",
    "             nvl(compile_secs,0)::NUMERIC(12,2) / 60 AS compile_minutes,\n",
    "             nvl(num_compile_segments,0) AS num_compile_segments,\n",
    "\t\t\t s.user_table_involved,\n",
    "\t\t\t NVL(s3.spectrum_object_count,0) AS spectrum_object_count,\n",
    "\t\t\t NVL(f.federated_object_count,0) AS federated_object_count\n",
    "        FROM stl_query\n",
    "\t\tINNER JOIN pg_catalog.pg_user pu on (stl_query.userid = pu.usesysid)\n",
    "        LEFT OUTER JOIN (SELECT query,\n",
    "                                SUM(DECODE (TRIM(SPLIT_PART (event,':',1)),'Very selective query filter',1,'Scanned a large number of deleted rows',2,'Nested Loop Join in the query plan',4,'Distributed a large number of rows across the network',8,'Broadcasted a large number of rows across the network',16,'Missing query planner statistics',32,0)) AS event\n",
    "                         FROM stl_alert_event_log\n",
    "                         WHERE event_time >= DATEADD(DAY,-7,CURRENT_DATE)\n",
    "                         GROUP BY query) AS alrt ON alrt.query = stl_query.query\n",
    "        LEFT OUTER JOIN (SELECT ut.xid,\n",
    "                                TRIM(SUBSTRING(text FROM STRPOS (UPPER(text),'SELECT'))) AS TEXT\n",
    "                         FROM stl_utilitytext ut\n",
    "                         WHERE SEQUENCE = 0\n",
    "                         AND   text ilike 'DECLARE%'\n",
    "                         GROUP BY text,\n",
    "                                  ut.xid) qrytext_cur ON (stl_query.xid = qrytext_cur.xid)\n",
    "        LEFT OUTER JOIN svl_query_metrics_summary sqms\n",
    "                     ON (sqms.userid = stl_query.userid AND sqms.query = stl_query.query)\n",
    "        LEFT OUTER JOIN (SELECT userid,\n",
    "                                xid,\n",
    "                                pid,\n",
    "                                query,\n",
    "                                MAX(datediff (ms,starttime,endtime)*1.0 / 1000) AS compile_secs,\n",
    "                                SUM(compile) AS num_compile_segments\n",
    "                         FROM svcs_compile\n",
    "                         GROUP BY userid,\n",
    "                                  xid,\n",
    "                                  pid,\n",
    "                                  query) c\n",
    "                     ON (c.userid = stl_query.userid\n",
    "                    AND c.xid = stl_query.xid\n",
    "                    AND c.pid = stl_query.pid\n",
    "                    AND c.query = stl_query.query)\n",
    "        LEFT OUTER JOIN (SELECT s.userid,\n",
    "                                s.query,\n",
    "                                LISTAGG(distinct TRIM(s.perm_table_name),', ') AS user_table_involved\n",
    "                           FROM stl_scan s\n",
    "                             INNER JOIN (SELECT DISTINCT(stv_tbl_perm.id) AS table_id\n",
    "                    ,TRIM(pg_database.datname) AS database_name\n",
    "                    ,TRIM(pg_namespace.nspname) AS schema_name\n",
    "                    ,TRIM(relname) AS table_name\n",
    "                FROM stv_tbl_perm\n",
    "              INNER JOIN pg_database on pg_database.oid = stv_tbl_perm.db_id\n",
    "              INNER JOIN pg_class on pg_class.oid = stv_tbl_perm.id\n",
    "              INNER JOIN pg_namespace on pg_namespace.oid = pg_class.relnamespace\n",
    "               WHERE schema_name NOT IN ('pg_internal', 'pg_catalog','pg_automv')) t ON (s.tbl = t.table_id)\n",
    "                          WHERE s.perm_table_name NOT IN ('Internal Worktable','S3')\n",
    "\t\t\t\t\t\t    AND s.perm_table_name NOT LIKE ('volt_tt%')\n",
    "                            AND t.schema_name NOT IN ('pg_internal', 'pg_catalog') \n",
    "                         GROUP BY s.userid,\n",
    "                                  s.query) s ON (stl_query.userid = s.userid AND stl_query.query = s.query)\t\n",
    "        LEFT OUTER JOIN (SELECT s.userid,\n",
    "                                s.query,\n",
    "                                COUNT(1) AS spectrum_object_count \n",
    "                           FROM svl_s3query_summary s \n",
    "\t\t\t\t\t\t  WHERE s.external_table_name NOT IN ('PG Subquery')\n",
    "                         GROUP BY s.userid,\n",
    "                                  s.query) s3 ON (stl_query.userid = s3.userid AND stl_query.query = s3.query)\n",
    "        LEFT OUTER JOIN (SELECT f.userid,\n",
    "                                f.query,\n",
    "                                COUNT(1) AS federated_object_count \n",
    "                           FROM svl_federated_query f \n",
    "                         GROUP BY f.userid,\n",
    "                                  f.query) f ON (stl_query.userid = f.userid AND stl_query.query = f.query)\t\t\t\t\t\t\t\t  \n",
    "      WHERE stl_query.userid <> 1\n",
    "\t    AND NVL(qrytext_cur.text,TRIM(querytxt)) NOT LIKE 'padb_fetch_sample:%' \n",
    "\t\tAND NVL(qrytext_cur.text,TRIM(querytxt)) NOT LIKE 'CREATE TEMP TABLE volt_tt_%'\n",
    "        AND stl_query.starttime >= DATEADD(DAY,-7,CURRENT_DATE))\n",
    "GROUP BY TRIM(dbname),\n",
    "         TRIM(db_username),\n",
    "         qry_md5,\n",
    "\t\t user_table_involved,\n",
    "         event\n",
    "ORDER BY avg_disk_spill_mb DESC, num_queries DESC LIMIT 50;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41396d1f-98bb-4bb2-9680-3073e1f56da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries[\"CopyPerformance\"]=\"\"\"SELECT a.endtime::DATE AS copy_date,\n",
    "       trim(d.region) AS aws_region,\n",
    "       trim(d.s3_bucket) AS s3_bucket,\n",
    "       trim(d.file_format) AS file_format,\n",
    "       trim(q.\"database\") as dbname,\n",
    "       a.tbl AS table_id,\n",
    "       trim(c.nspname) AS namespace,\n",
    "       trim(b.relname) AS table_name,\n",
    "       SUM(a.rows_inserted) AS rows_inserted,\n",
    "       SUM(d.distinct_files) AS files_scanned,\n",
    "       SUM(d.mb_scanned) AS mb_scanned,\n",
    "       (SUM(d.distinct_files)::NUMERIC(19,3) / COUNT(DISTINCT a.query)::NUMERIC(19,3))::NUMERIC(19,3) AS avg_files_per_copy,\n",
    "       (SUM(d.mb_scanned) / SUM(d.distinct_files)::NUMERIC(19,3))::NUMERIC(19,3) AS avg_file_size_mb,\n",
    "       MAX(d.files_compressed) AS files_compressed,\n",
    "       MAX(cluster_slice_count) AS cluster_slice_count,\n",
    "       AVG(d.used_slice_count) AS avg_used_slice_count,\n",
    "       COUNT(DISTINCT a.query) no_of_copy,\n",
    "       MAX(a.query) AS sample_query,\n",
    "       ROUND((SUM(d.mb_scanned)*1024 *1000000.0 / SUM(d.load_micro)),4) AS scan_rate_kbps,\n",
    "       ROUND((SUM(a.rows_inserted)*1000000.0 / SUM(a.insert_micro)),4) AS insert_rate_rows_per_second,\n",
    "       ROUND(SUM(d.copy_duration_micro)/1000000.0,4) AS total_copy_time_secs,\n",
    "       ROUND(AVG(d.copy_duration_micro)/1000000.0,4) AS avg_copy_time_secs,\n",
    "       ROUND(SUM(d.compression_micro)/1000000.0,4) AS total_compression_time_secs,\n",
    "       ROUND(AVG(d.compression_micro)/1000000.0,4) AS avg_compression_time_secs,       \n",
    "       SUM(d.total_transfer_retries) AS total_transfer_retries,\n",
    "       SUM(d.distinct_error_files) AS distinct_error_files,\n",
    "       SUM(d.load_error_count) AS load_error_count\n",
    "FROM (SELECT query,\n",
    "             tbl,\n",
    "             SUM(ROWS) AS rows_inserted,\n",
    "             MAX(endtime) AS endtime,\n",
    "             datediff('microsecond',MIN(starttime),MAX(endtime)) AS insert_micro\n",
    "      FROM stl_insert\n",
    "      GROUP BY query,\n",
    "               tbl) a,\n",
    "     pg_class b,\n",
    "     pg_namespace c,\n",
    "     (SELECT b.region,\n",
    "             l.file_format,\n",
    "             b.query,\n",
    "             b.bucket as s3_bucket,\n",
    "             COUNT(DISTINCT b.bucket||b.key) AS distinct_files,\n",
    "             COUNT(DISTINCT b.slice) as used_slice_count,\n",
    "             SUM(b.transfer_size) / 1024 / 1024 AS mb_scanned,\n",
    "             SUM(b.transfer_time) AS load_micro,\n",
    "             SUM(b.compression_time) AS compression_micro,\n",
    "             datediff('microsecond',MIN('2000-01-01'::timestamp + (start_time/1000000.0)* interval '1 second'),MAX('2000-01-01'::timestamp + (end_time/1000000.0)* interval '1 second')) AS copy_duration_micro,\n",
    "             SUM(b.retries) as total_transfer_retries,\n",
    "             SUM(nvl(se.distinct_error_files,0)) AS distinct_error_files,\n",
    "             SUM(nvl(se.load_error_count,0)) AS load_error_count,\n",
    "             CASE WHEN SUM(b.transfer_size) = SUM(b.data_size) then 'N' else  'Y' end AS files_compressed\n",
    "      FROM stl_s3client b\n",
    "      INNER JOIN (select userid, query, MAX(file_format) as file_format FROM stl_load_commits GROUP BY userid, query) l ON (l.userid = b.userid and l.query = b.query)\n",
    "      LEFT OUTER JOIN (select userid, query, COUNT(DISTINCT bucket||key) AS distinct_error_files, COUNT(1) AS load_error_count FROM stl_s3client_error GROUP BY userid, query) se ON (se.userid = b.userid and se.query = b.query)\n",
    "      WHERE b.http_method = 'GET'\n",
    "      GROUP BY b.region,l.file_format,b.query,b.bucket) d,\n",
    "     stl_query q,\n",
    "     (SELECT COUNT(1) AS cluster_slice_count FROM stv_slices)\n",
    "WHERE a.tbl = b.oid\n",
    "AND   b.relnamespace = c.oid\n",
    "AND   d.query = a.query\n",
    "AND   a.query = q.query\n",
    "AND   lower(q.querytxt) LIKE '%copy %'\n",
    "GROUP BY 1,2,3,4,5,6,7,8\n",
    "ORDER BY 9 DESC, 21 DESC, 1 DESC\n",
    " LIMIT 50;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44008cb4-d313-4699-be71-d3612d3233ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries[\"SpectrumPerformance\"]=\"\"\"SELECT trim(et.schemaname) AS namespace,\n",
    "       trim(et.tablename) AS external_table_name,\n",
    "       trim(lq.file_format) AS file_format,\n",
    "       trim(l.s3_bucket) AS s3_bucket,\n",
    "       CASE\n",
    "         WHEN et.compressed = 1 THEN 'Y'\n",
    "         ELSE 'N'\n",
    "       END AS is_table_compressed,\n",
    "       CASE\n",
    "         WHEN lq.is_partitioned = 't' THEN 'Y'\n",
    "         ELSE 'N'\n",
    "       END AS is_table_partitioned,\n",
    "       CASE\n",
    "         WHEN l.avg_file_splits > 1 THEN 'Y'\n",
    "         ELSE 'N'\n",
    "       END AS is_file_spittable,\n",
    "       MAX(nvl (ep.external_table_partition_count,0)) AS external_table_partition_count,\n",
    "       COUNT(1) total_query_count,\n",
    "       SUM(CASE WHEN lp.qualified_partitions < ep.external_table_partition_count THEN 1 ELSE 0 END) total_query_using_Partition_Pruning_count,\n",
    "       ROUND(SUM(CASE WHEN lp.qualified_partitions < ep.external_table_partition_count THEN 1 ELSE 0 END) / COUNT(1)::NUMERIC(38,4)*100.0,4) AS pct_of_query_using_Partition_Pruning,\n",
    "       nvl(AVG(CASE WHEN lp.qualified_partitions != 0 THEN lp.qualified_partitions END),0) AS avg_Qualified_Partitions,\n",
    "       nvl(AVG(CASE WHEN lp.qualified_partitions != 0 THEN lp.avg_assigned_partitions END),0) AS avg_Assigned_Partitions,\n",
    "       ROUND(nvl(AVG(CASE WHEN lp.qualified_partitions != 0 THEN lq.avg_request_parallelism END),0),4) AS avg_Parallelism,\n",
    "       nvl(AVG(CASE WHEN lp.qualified_partitions != 0 THEN lq.files END),0) AS avg_Files,\n",
    "       AVG(lq.splits) AS avg_Split,\n",
    "       ROUND(AVG(l.avg_max_file_size_mb),4) AS avg_max_file_size_mb,\n",
    "       ROUND(AVG(l.avg_file_size_mb),4) AS avg_file_size_mb,\n",
    "       ROUND(AVG(elapsed / 1000000::NUMERIC(38,4)),4) avg_Elapsed_sec,\n",
    "       ROUND(SUM(elapsed / 1000000::NUMERIC(38,4)),4) Total_Elapsed_sec,\n",
    "       SUM(nvl(r.spectrum_scan_error_count,0)) AS total_spectrum_scan_error_count,\n",
    "       AVG(nvl(r.spectrum_scan_error_count,0)) AS avg_spectrum_scan_error_count,\n",
    "       SUM(CASE WHEN lp.qualified_partitions = 0 THEN 1 ELSE 0 END) Queries_Using_No_S3Files\n",
    "FROM svl_s3query_summary lq\n",
    "  INNER JOIN stl_query q\n",
    "          ON (q.userid = lq.userid\n",
    "         AND q.query = lq.query\n",
    "         AND q.xid = lq.xid\n",
    "         AND q.pid = lq.pid)\n",
    "  INNER JOIN svv_external_tables et ON (q.database || '_' || et.schemaname || '_' || et.tablename = replace (replace (lq.external_table_name,'S3 Scan ',''),'S3 Subquery ',''))\n",
    "  LEFT OUTER JOIN (SELECT schemaname,\n",
    "                          tablename,\n",
    "                          COUNT(1) AS external_table_partition_count\n",
    "                   FROM svv_external_partitions\n",
    "                   GROUP BY schemaname,\n",
    "                            tablename) ep\n",
    "               ON (ep.schemaname = et.schemaname\n",
    "              AND ep.tablename = et.tablename)\n",
    "  LEFT OUTER JOIN svl_s3partition_summary lp ON lq.query = lp.query\n",
    "  LEFT OUTER JOIN (SELECT query,\n",
    "                          bucket AS s3_bucket,\n",
    "                          AVG(max_file_size / 1000000.0) AS avg_max_file_size_mb,\n",
    "                          AVG(avg_file_size / 1000000.0) AS avg_file_size_mb,\n",
    "                          AVG(generated_splits) AS avg_file_splits\n",
    "                   FROM svl_s3list\n",
    "                   GROUP BY query,\n",
    "                            bucket) l ON (lq.query = l.query)\n",
    "  LEFT OUTER JOIN (SELECT query,\n",
    "                          userid,\n",
    "                          count(1) AS spectrum_scan_error_count\n",
    "                   FROM svl_spectrum_scan_error\n",
    "                   GROUP BY query, userid) r ON (lq.query = r.query and lq.userid = r.userid)\n",
    "WHERE lq.starttime >= dateadd(day,- 7,CURRENT_DATE)\n",
    "AND   lq.aborted = 0\n",
    "GROUP BY 1,2,3,4,5,6,7\n",
    "ORDER BY Total_Elapsed_sec DESC LIMIT 50;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a62c356e-9ac9-4795-afd4-89a1b93c0205",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries[\"DataShareProducerObject\"]=\"\"\"WITH shared_table AS\n",
    "(\n",
    "  SELECT t.schema_name|| '.' ||t.table_name AS object_name,\n",
    "         MAX(t.table_rows) AS table_rows,\n",
    "         MAX(v.eventtime) AS last_vacuum_date,\n",
    "         MAX(CASE WHEN v.is_recluster = 0 THEN 'N' WHEN v.is_recluster = 1 THEN 'Y' ELSE NULL END) AS is_last_vacuum_recluster,\n",
    "         MAX(i.num_insert_operation) AS num_insert_operation,\n",
    "         MAX(i.total_inserted_rows) AS total_inserted_rows,\n",
    "         MAX(i.last_insert_date) AS last_insert_date,\n",
    "         MAX(d.num_delete_operation) AS num_delete_operation,\n",
    "         MAX(d.total_deleted_rows) AS total_deleted_rows,\n",
    "         MAX(d.last_delete_date) AS last_delete_date\n",
    "  FROM (SELECT DISTINCT (stv_tbl_perm.id) table_id,\n",
    "               TRIM(pg_database.datname) AS \"database\",\n",
    "               TRIM(pg_namespace.nspname) AS schema_name,\n",
    "               TRIM(relname) AS table_name,\n",
    "               reltuples::bigint AS table_rows\n",
    "        FROM stv_tbl_perm\n",
    "          JOIN pg_database ON pg_database.oid = stv_tbl_perm.db_id\n",
    "          JOIN pg_class ON pg_class.oid = stv_tbl_perm.id\n",
    "          JOIN pg_namespace ON pg_namespace.oid = pg_class.relnamespace\n",
    "        WHERE schema_name NOT IN ('pg_internal','pg_catalog','pg_automv')) t\n",
    "    LEFT OUTER JOIN stl_vacuum v\n",
    "                 ON (t.table_id = v.table_id\n",
    "                AND v.status NOT LIKE 'Skip%')\n",
    "    LEFT OUTER JOIN (SELECT tbl,\n",
    "                            COUNT(1) AS num_insert_operation,\n",
    "                            SUM(ROWS) AS total_inserted_rows,\n",
    "                            MAX(endtime) AS last_insert_date\n",
    "                     FROM stl_insert\n",
    "                     GROUP BY 1) i ON (i.tbl = t.table_id)\n",
    "    LEFT OUTER JOIN (SELECT tbl,\n",
    "                            COUNT(1) AS num_delete_operation,\n",
    "                            SUM(ROWS) AS total_deleted_rows,\n",
    "                            MAX(endtime) AS last_delete_date\n",
    "                     FROM stl_delete\n",
    "                     GROUP BY 1) d ON (d.tbl = t.table_id)\n",
    "  GROUP BY 1\n",
    ")\n",
    "SELECT d.share_type,\n",
    "       d.share_name,\n",
    "       case when d.include_new = true then 'True' when d.include_new = false then 'False' else null end as include_new,\n",
    "       d.producer_account,\n",
    "       d.object_type,\n",
    "       replace(substring(d.object_name,1,strpos (d.object_name,'.')),'.','') AS namespace,\n",
    "       substring(d.object_name,strpos (d.object_name,'.') +1) AS object_name,\n",
    "       ti.table_rows,\n",
    "       ti.last_vacuum_date,\n",
    "       ti.is_last_vacuum_recluster,\n",
    "       ti.num_insert_operation,\n",
    "       ti.total_inserted_rows,\n",
    "       ti.last_insert_date,\n",
    "       ti.num_delete_operation,\n",
    "       ti.total_deleted_rows,\n",
    "       ti.last_delete_date,\n",
    "       CASE\n",
    "         WHEN mi.is_stale = 't' THEN 'Y'\n",
    "         WHEN mi.is_stale = 'f' THEN 'N'\n",
    "         ELSE NULL\n",
    "       END AS is_mv_stale,\n",
    "       CASE\n",
    "         WHEN mi.state = 1 THEN 'Y'\n",
    "         WHEN mi.state <> 1 THEN 'N'\n",
    "         ELSE NULL\n",
    "       END AS is_mv_incremental_refresh,\n",
    "       CASE\n",
    "         WHEN mi.autorefresh = 1 THEN 'Y'\n",
    "         WHEN mi.autorefresh <> 1 THEN 'N'\n",
    "         ELSE NULL\n",
    "       END AS is_mv_auto_refresh\n",
    "FROM svv_datashare_objects d\n",
    "  LEFT OUTER JOIN shared_table ti ON (d.object_name = ti.object_name)\n",
    "  LEFT OUTER JOIN stv_mv_info mi ON (d.object_name = mi.schema|| '.' ||mi.name)\n",
    "WHERE d.share_type = 'OUTBOUND'\n",
    "ORDER BY 2,3,5,4;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6144fe86-8ca9-49ba-b3c6-b957428244bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries[\"DataShareConsumerUsage\"]=\"\"\"with consumer_activity as \n",
    "(\n",
    "select uc.userid\n",
    "      ,u.usename as db_username\n",
    "      ,uc.pid\n",
    "      ,uc.xid\n",
    "      ,min(uc.recordtime) as request_start_date\n",
    "      ,max(uc.recordtime) as request_end_date\n",
    "      ,datediff('milliseconds',min(uc.recordtime),max(uc.recordtime))::NUMERIC(38,4) / 1000 as request_duration_secs\n",
    "      ,nvl(count(distinct uc.transaction_uid),0) as unique_transaction\n",
    "      ,nvl(count(uc.request_id),0) as total_usage_consumer_count\n",
    "      ,sum(case when trim(uc.error) = '' then 0 else 1 end) as request_error_count\n",
    "  from svl_datashare_usage_consumer uc\n",
    "  inner join pg_user u on (u.usesysid = uc.userid)\n",
    "group by 1,2,3,4\n",
    ")\n",
    ",consumer_query as (\n",
    "select trim(q.\"database\") as dbname\n",
    "      ,trim(cu.db_username) as db_username\n",
    "      ,cu.request_start_date::date as request_date\n",
    "      ,cu.request_duration_secs\n",
    "      ,datediff('milliseconds',cu.request_end_date,q.starttime)::NUMERIC(38,4) / 1000  as request_interval_secs\n",
    "      ,datediff('milliseconds',q.starttime,q.endtime)::NUMERIC(38,4) / 1000 as query_execution_secs\n",
    "      ,datediff('milliseconds',request_start_date,q.endtime)::NUMERIC(38,4) / 1000 as total_execution_secs\n",
    "      ,q.query\n",
    "      ,cu.unique_transaction\n",
    "      ,cu.total_usage_consumer_count\n",
    "      ,cu.request_error_count      \n",
    "from consumer_activity cu \n",
    "inner join stl_query q on (q.xid = cu.xid and q.pid = cu.pid and q.userid = cu.userid)\n",
    ")\n",
    ",consumer_query_aggregate as (\n",
    "select cq.request_date\n",
    "      ,cq.dbname\n",
    "      ,cq.db_username\n",
    "\t  ,avg(cq.request_duration_secs) as avg_request_duration_secs\n",
    "      ,sum(cq.request_duration_secs) as total_request_duration_secs\n",
    "\t  ,avg(cq.request_interval_secs) as avg_request_interval_secs\n",
    "      ,sum(cq.request_interval_secs) as total_request_interval_secs\n",
    "\t  ,avg(cq.query_execution_secs) as avg_query_execution_secs\n",
    "      ,sum(cq.query_execution_secs) as total_query_execution_secs\n",
    "\t  ,avg(cq.total_execution_secs) as avg_execution_secs\n",
    "      ,sum(cq.total_execution_secs) as total_execution_secs\n",
    "      ,count(cq.query) as query_count\n",
    "      ,sum(cq.unique_transaction) as total_unique_transaction\n",
    "      ,sum(cq.total_usage_consumer_count) as total_usage_consumer_count\n",
    "      ,sum(cq.request_error_count) as total_request_error_count \n",
    "  from consumer_query cq\n",
    "group by 1,2,3\n",
    ")\n",
    ",consumer_query_request_percentile AS (\n",
    "SELECT cq.request_date\n",
    "      ,cq.dbname\n",
    "      ,cq.db_username\n",
    "\t  ,percentile_cont(0.8) within GROUP ( ORDER BY request_duration_secs) AS p80_request_sec\n",
    "\t  ,percentile_cont(0.9) within GROUP ( ORDER BY request_duration_secs) AS p90_request_sec\n",
    "\t  ,percentile_cont(0.99) within GROUP ( ORDER BY request_duration_secs) AS p99_request_sec\n",
    "  from consumer_query cq\n",
    "group by 1,2,3\n",
    ")\n",
    "select cqa.request_date\n",
    "      ,cqa.dbname\n",
    "      ,cqa.db_username\n",
    "      ,cqa.query_count\n",
    "\t  ,cqa.avg_query_execution_secs\n",
    "      ,cqa.total_query_execution_secs\n",
    "\t  ,cqa.avg_execution_secs\n",
    "      ,cqa.total_execution_secs\n",
    "\t  ,cqa.avg_request_duration_secs\n",
    "\t  ,cqrp.p80_request_sec\n",
    "\t  ,cqrp.p90_request_sec\n",
    "\t  ,cqrp.p99_request_sec\n",
    "      ,cqa.total_request_duration_secs\n",
    "\t  ,cqa.avg_request_interval_secs\n",
    "      ,cqa.total_request_interval_secs\t  \n",
    "      ,cqa.total_unique_transaction\n",
    "      ,cqa.total_usage_consumer_count\n",
    "      ,cqa.total_request_error_count\n",
    "  from consumer_query_aggregate cqa\n",
    "  inner join consumer_query_request_percentile cqrp on (cqa.request_date = cqrp.request_date and cqa.dbname = cqrp.dbname and cqa.db_username = cqrp.db_username)\n",
    "order by 1,2,3;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c147826b-a567-402a-bf7b-a678ddabb6a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries[\"ATOWorkerActions\"]=\"\"\"WITH latest_worker_action AS\n",
    "(\n",
    "  SELECT table_id,\n",
    "         TYPE,\n",
    "         trim(status) as status,\n",
    "         eventtime,\n",
    "         ROW_NUMBER() OVER (PARTITION BY table_id,TYPE ORDER BY eventtime DESC) AS rnum\n",
    "  FROM svl_auto_worker_action w\n",
    ")\n",
    "SELECT trim(t.database_name) AS dbname,\n",
    "       t.schema_name AS namespace,\n",
    "       t.table_name,\n",
    "       w.type,\n",
    "       w.status AS latest_status,\n",
    "       w.eventtime\n",
    "FROM latest_worker_action w\n",
    "  INNER JOIN (SELECT DISTINCT(stv_tbl_perm.id) AS table_id\n",
    "                    ,TRIM(pg_database.datname) AS database_name\n",
    "                    ,TRIM(pg_namespace.nspname) AS schema_name\n",
    "                    ,TRIM(relname) AS table_name\n",
    "                FROM stv_tbl_perm\n",
    "              INNER JOIN pg_database on pg_database.oid = stv_tbl_perm.db_id\n",
    "              INNER JOIN pg_class on pg_class.oid = stv_tbl_perm.id\n",
    "              INNER JOIN pg_namespace on pg_namespace.oid = pg_class.relnamespace\n",
    "               WHERE schema_name NOT IN ('pg_internal', 'pg_catalog','pg_automv')) t\n",
    "          ON (t.table_id = w.table_id AND w.rnum = 1)\n",
    "ORDER BY 1,2,3,4;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70bf8b2c-c780-48ee-8c93-a1e95fec2c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries[\"WorkloadEvaluation\"] = {}\n",
    "queries[\"WorkloadEvaluation\"][\"text\"] =\"\"\"WITH hour_list AS\n",
    "(\n",
    "  SELECT DATE_TRUNC('m',starttime) start_hour,\n",
    "         dateadd('m',1,start_hour) AS end_hour\n",
    "  FROM stl_query q\n",
    "  WHERE starttime >= (getdate() -7)\n",
    "  GROUP BY 1\n",
    "),\n",
    "scan_sum AS\n",
    "(\n",
    "  SELECT query,\n",
    "         segment,\n",
    "         SUM(bytes) AS bytes\n",
    "  FROM stl_scan\n",
    "  WHERE userid > 1\n",
    "  GROUP BY query,\n",
    "           segment\n",
    "),\n",
    "scan_list AS\n",
    "(\n",
    "  SELECT query,\n",
    "         MAX(bytes) AS max_scan_bytes\n",
    "  FROM scan_sum\n",
    "  GROUP BY query\n",
    "),\n",
    "query_list AS\n",
    "(\n",
    "  SELECT w.query,\n",
    "         exec_start_time,\n",
    "         exec_end_time,\n",
    "         ROUND(total_exec_time / 1000 / 1000.0,3) AS exec_sec,\n",
    "         max_scan_bytes,\n",
    "         CASE\n",
    "           WHEN max_scan_bytes < 100000000 THEN 'small'\n",
    "           WHEN max_scan_bytes BETWEEN 100000000 AND 500000000000 THEN 'medium'\n",
    "           WHEN max_scan_bytes > 500000000000 THEN 'large'\n",
    "         END AS size_type\n",
    "  FROM stl_wlm_query w,\n",
    "       scan_list sc\n",
    "  WHERE sc.query = w.query\n",
    "),\n",
    "workload_exec_seconds  AS\n",
    "(\n",
    "select \n",
    "count(*) as query_cnt, \n",
    "SUM(CASE WHEN size_type = 'small' THEN  exec_sec  ELSE 0 END) AS small_workload_exec_sec_sum,\n",
    "SUM(CASE WHEN size_type = 'medium' THEN  exec_sec  ELSE 0 END) AS medium_workload_exec_sec_sum,\n",
    "SUM(CASE WHEN size_type = 'large' THEN  exec_sec  ELSE 0 END) AS large_workload_exec_sec_sum,\n",
    "  \n",
    "AVG(CASE WHEN size_type = 'small' THEN  exec_sec  ELSE 0 END) AS small_workload_exec_sec_avg,\n",
    "AVG(CASE WHEN size_type = 'medium' THEN  exec_sec  ELSE 0 END) AS medium_workload_exec_sec_avg,\n",
    "AVG(CASE WHEN size_type = 'large' THEN  exec_sec  ELSE 0 END) AS large_workload_exec_sec_avg,\n",
    "  \n",
    "MAX(CASE WHEN size_type = 'small' THEN  exec_sec  ELSE 0 END) AS small_workload_exec_sec_max,\n",
    "MAX(CASE WHEN size_type = 'medium' THEN  exec_sec  ELSE 0 END) AS medium_workload_exec_sec_max,\n",
    "MAX(CASE WHEN size_type = 'large' THEN  exec_sec  ELSE 0 END) AS large_workload_exec_sec_max,\n",
    "  \n",
    "  MIN(CASE WHEN size_type = 'small' THEN  exec_sec  ELSE 0 END) AS small_workload_exec_sec_min,\n",
    "MIN(CASE WHEN size_type = 'medium' THEN  exec_sec  ELSE 0 END) AS medium_workload_exec_sec_min,\n",
    "MIN(CASE WHEN size_type = 'large' THEN  exec_sec  ELSE 0 END) AS large_workload_exec_sec_min,\n",
    "  \n",
    "AVG(CASE WHEN size_type = 'small' THEN  max_scan_bytes  ELSE 0 END) AS small_workload_max_scan_bytes_avg,\n",
    "AVG(CASE WHEN size_type = 'medium' THEN  max_scan_bytes  ELSE 0 END) AS medium_workload_max_scan_bytes_avg,\n",
    "AVG(CASE WHEN size_type = 'large' THEN  max_scan_bytes  ELSE 0 END) AS large_workload_max_scan_bytes_avg,\n",
    "\n",
    "  (small_workload_exec_sec_sum+medium_workload_exec_sec_sum+large_workload_exec_sec_sum) as total_workload_exec_sec_sum,\n",
    "small_workload_exec_sec_sum/(total_workload_exec_sec_sum*1.00) as Small_workload_perc,\n",
    "medium_workload_exec_sec_sum/(total_workload_exec_sec_sum*1.00) as Medium_workload_perc,\n",
    "large_workload_exec_sec_sum/(total_workload_exec_sec_sum*1.00) as Large_workload_perc\n",
    "from query_list\n",
    ")\n",
    ",query_list_2 AS\n",
    "(\n",
    "  SELECT start_hour,\n",
    "         query,\n",
    "         size_type,\n",
    "         max_scan_bytes,\n",
    "         exec_sec,\n",
    "         exec_start_time,\n",
    "         exec_end_time\n",
    "  FROM hour_list h,\n",
    "       query_list q\n",
    "  WHERE exec_start_time BETWEEN start_hour AND end_hour\n",
    "  OR    exec_end_time BETWEEN start_hour AND end_hour\n",
    "  OR    (exec_start_time < start_hour AND exec_end_time > end_hour)\n",
    ")\n",
    "\n",
    ",hour_list_agg AS\n",
    "(\n",
    "  SELECT start_hour,\n",
    "         SUM(CASE WHEN size_type = 'small' THEN 1 ELSE 0 END) AS small_query_cnt,\n",
    "         SUM(CASE WHEN size_type = 'medium' THEN 1 ELSE 0 END) AS medium_query_cnt,\n",
    "         SUM(CASE WHEN size_type = 'large' THEN 1 ELSE 0 END) AS large_query_cnt,\n",
    "         COUNT(*) AS tot_query_cnt\n",
    "  FROM query_list_2\n",
    "  GROUP BY start_hour\n",
    ") \n",
    ",utilization_perc AS\n",
    "(\n",
    "SELECT trunc(start_hour) AS sample_date,\n",
    "       ROUND(100*SUM(CASE WHEN tot_query_cnt > 0 THEN 1 ELSE 0 END) / 1440.0,1) AS all_query_activite_perc,\n",
    "       ROUND(100*SUM(CASE WHEN small_query_cnt > 0 THEN 1 ELSE 0 END) / 1440.0,1) AS small_query_activite_perc,\n",
    "       ROUND(100*SUM(CASE WHEN medium_query_cnt > 0 THEN 1 ELSE 0 END) / 1440.0,1) AS medium_query_activite_perc,\n",
    "       ROUND(100*SUM(CASE WHEN large_query_cnt > 0 THEN 1 ELSE 0 END) / 1440.0,1) AS large_query_activite_perc,\n",
    "       MIN(start_hour) AS start_hour,\n",
    "       MAX(start_hour) AS end_hour\n",
    "FROM hour_list_agg\n",
    "GROUP BY 1\n",
    ")\n",
    "\n",
    ",activity_perc as \n",
    "(\n",
    "Select avg(small_query_activite_perc) AS AVG_small_query_activity_perc, \n",
    "avg(medium_query_activite_perc) AS AVG_medium_query_activity_perc,\n",
    "avg(large_query_activite_perc) AS AVG_large_query_activity_perc\n",
    "FROM utilization_perc\n",
    ")\n",
    "\n",
    ",mincount AS\n",
    "(\n",
    "SELECT trunc(start_hour) AS sample_date,\n",
    "       SUM(CASE WHEN tot_query_cnt > 0 THEN 1 ELSE 0 END) AS tot_query_minute,\n",
    "       SUM(CASE WHEN small_query_cnt > 0 THEN 1 ELSE 0 END) AS small_query_minute,\n",
    "       SUM(CASE WHEN medium_query_cnt > 0 THEN 1 ELSE 0 END) AS medium_query_minute,\n",
    "       SUM(CASE WHEN large_query_cnt > 0 THEN 1 ELSE 0 END) AS large_query_minute,\n",
    "       MIN(start_hour) AS start_hour,\n",
    "       MAX(start_hour) AS end_hour\n",
    "FROM hour_list_agg\n",
    "GROUP BY 1\n",
    "),avgmincount AS\n",
    "(\n",
    "Select avg(small_query_minute) avg_small_query_minute, avg(medium_query_minute) avg_medium_query_minute, avg(large_query_minute) avg_large_query_minute\n",
    "  from mincount\n",
    ")\n",
    ",final_output AS \n",
    "(\n",
    "Select   small_workload_perc , medium_workload_perc,    large_workload_perc,    \n",
    "  avg_small_query_activity_perc,    avg_medium_query_activity_perc,    avg_large_query_activity_perc,    \n",
    "  avg_small_query_minute,    avg_medium_query_minute,    avg_large_query_minute,\n",
    "  \n",
    "  small_workload_exec_sec_avg, medium_workload_exec_sec_avg, large_workload_exec_sec_avg,\n",
    "  small_workload_exec_sec_max, medium_workload_exec_sec_max, large_workload_exec_sec_max,\n",
    "  small_workload_exec_sec_min, medium_workload_exec_sec_min, large_workload_exec_sec_min,\n",
    "   total_query_cnt, total_small_query_cnt, \n",
    "  total_medium_query_cnt,total_large_query_cnt, \n",
    "  small_workload_max_scan_bytes_avg, \n",
    "  medium_workload_max_scan_bytes_avg, \n",
    "  large_workload_max_scan_bytes_avg\n",
    "from activity_perc a, avgmincount b, workload_exec_seconds c, (  select count(*) as total_query_cnt, sum(case when size_type = 'small' then 1 else 0 end) as  total_small_query_cnt, \n",
    "                                                              sum(case when size_type = 'medium' then 1 else 0 end) as  total_medium_query_cnt, \n",
    "                                                              sum(case when size_type = 'large' then 1 else 0 end) as  total_large_query_cnt\n",
    "                                                              from query_list )  d\n",
    "where 1=1\n",
    "\n",
    ")\n",
    "select workloadtype, (perc_of_total_workload*100.00) as perc_of_total_workload, perc_duration_in_day, Total_query_minutes_in_day \n",
    ",workload_exec_sec_avg, workload_exec_sec_min, workload_exec_sec_max,query_cnt,scan_bytes_avg\n",
    "from\n",
    "(\n",
    "select \n",
    "'Small' as workloadtype,\n",
    "small_workload_perc as perc_of_total_workload,\n",
    "avg_small_query_activity_perc as perc_duration_in_day,\n",
    "avg_small_query_minute as Total_query_minutes_in_day, \n",
    " small_workload_exec_sec_avg as  workload_exec_sec_avg,\n",
    "  small_workload_exec_sec_max as  workload_exec_sec_max, \n",
    "   small_workload_exec_sec_min as  workload_exec_sec_min,\n",
    " total_small_query_cnt as query_cnt, \n",
    "  small_workload_max_scan_bytes_avg as scan_bytes_avg,\n",
    " 1 as id\n",
    "from final_output\n",
    "union\n",
    "select \n",
    "'Meduim' as workloadtype,\n",
    "medium_workload_perc as perc_of_total_workload,\n",
    "avg_medium_query_activity_perc as perc_duration_in_day,\n",
    "avg_medium_query_minute as Total_query_minutes_in_day ,\n",
    "  medium_workload_exec_sec_avg as  workload_exec_sec_avg,\n",
    "    medium_workload_exec_sec_max as  workload_exec_sec_max, \n",
    "   medium_workload_exec_sec_min as  workload_exec_sec_min,\n",
    " total_medium_query_cnt as query_cnt, \n",
    "  medium_workload_max_scan_bytes_avg as scan_bytes_avg,\n",
    "  2 as id\n",
    "from final_output\n",
    "union\n",
    "select \n",
    "'Large' as workloadtype,\n",
    "large_workload_perc as perc_of_total_workload,\n",
    "avg_large_query_activity_perc as perc_duration_in_day,\n",
    "avg_large_query_minute as Total_query_minutes_in_day, \n",
    "  large_workload_exec_sec_avg as  workload_exec_sec_avg,\n",
    "   large_workload_exec_sec_max as  workload_exec_sec_max, \n",
    "   large_workload_exec_sec_min as  workload_exec_sec_min,\n",
    " total_large_query_cnt as query_cnt, \n",
    "  large_workload_max_scan_bytes_avg as scan_bytes_avg,\n",
    "  3 as id\n",
    " from final_output\n",
    "  ) a order by id asc;\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e602df0-b578-4966-96c8-811ba5339137",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prompting models with Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10f26552-02a3-4167-9b6d-76c47382c267",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade -q boto3\n",
    "!pip install --upgrade -q botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bbf03e23-c867-4a8d-9feb-a7a9e70040cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-east-1'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, boto3\n",
    "\n",
    "session = boto3.session.Session()\n",
    "session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95e21114-e4d4-4bfe-bab9-08a67155fe7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "bedrock = session.client(\n",
    "    service_name=\"bedrock\",\n",
    "    region_name=session.region_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f65e4a8-fc7b-4632-93b3-684f108f1ecc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-tg1-large',\n",
       "  'modelId': 'amazon.titan-tg1-large',\n",
       "  'modelName': 'Titan Text Large',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-image-generator-v1:0',\n",
       "  'modelId': 'amazon.titan-image-generator-v1:0',\n",
       "  'modelName': 'Titan Image Generator G1',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['IMAGE'],\n",
       "  'customizationsSupported': ['FINE_TUNING'],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-image-generator-v1',\n",
       "  'modelId': 'amazon.titan-image-generator-v1',\n",
       "  'modelName': 'Titan Image Generator G1',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['IMAGE'],\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-g1-text-02',\n",
       "  'modelId': 'amazon.titan-embed-g1-text-02',\n",
       "  'modelName': 'Titan Text Embeddings v2',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['EMBEDDING'],\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-lite-v1:0:4k',\n",
       "  'modelId': 'amazon.titan-text-lite-v1:0:4k',\n",
       "  'modelName': 'Titan Text G1 - Lite',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': ['FINE_TUNING', 'CONTINUED_PRE_TRAINING'],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-lite-v1',\n",
       "  'modelId': 'amazon.titan-text-lite-v1',\n",
       "  'modelName': 'Titan Text G1 - Lite',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-express-v1:0:8k',\n",
       "  'modelId': 'amazon.titan-text-express-v1:0:8k',\n",
       "  'modelName': 'Titan Text G1 - Express',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': ['FINE_TUNING', 'CONTINUED_PRE_TRAINING'],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-express-v1',\n",
       "  'modelId': 'amazon.titan-text-express-v1',\n",
       "  'modelName': 'Titan Text G1 - Express',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1:2:8k',\n",
       "  'modelId': 'amazon.titan-embed-text-v1:2:8k',\n",
       "  'modelName': 'Titan Embeddings G1 - Text',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['EMBEDDING'],\n",
       "  'responseStreamingSupported': False,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1',\n",
       "  'modelId': 'amazon.titan-embed-text-v1',\n",
       "  'modelName': 'Titan Embeddings G1 - Text',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['EMBEDDING'],\n",
       "  'responseStreamingSupported': False,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-image-v1:0',\n",
       "  'modelId': 'amazon.titan-embed-image-v1:0',\n",
       "  'modelName': 'Titan Multimodal Embeddings G1',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['EMBEDDING'],\n",
       "  'customizationsSupported': ['FINE_TUNING'],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-image-v1',\n",
       "  'modelId': 'amazon.titan-embed-image-v1',\n",
       "  'modelName': 'Titan Multimodal Embeddings G1',\n",
       "  'providerName': 'Amazon',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['EMBEDDING'],\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/stability.stable-diffusion-xl',\n",
       "  'modelId': 'stability.stable-diffusion-xl',\n",
       "  'modelName': 'SDXL 0.8',\n",
       "  'providerName': 'Stability AI',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['IMAGE'],\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'LEGACY'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/stability.stable-diffusion-xl-v0',\n",
       "  'modelId': 'stability.stable-diffusion-xl-v0',\n",
       "  'modelName': 'SDXL 0.8',\n",
       "  'providerName': 'Stability AI',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['IMAGE'],\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'LEGACY'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/stability.stable-diffusion-xl-v1:0',\n",
       "  'modelId': 'stability.stable-diffusion-xl-v1:0',\n",
       "  'modelName': 'SDXL 1.0',\n",
       "  'providerName': 'Stability AI',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['IMAGE'],\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/stability.stable-diffusion-xl-v1',\n",
       "  'modelId': 'stability.stable-diffusion-xl-v1',\n",
       "  'modelName': 'SDXL 1.0',\n",
       "  'providerName': 'Stability AI',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['IMAGE'],\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-grande-instruct',\n",
       "  'modelId': 'ai21.j2-grande-instruct',\n",
       "  'modelName': 'J2 Grande Instruct',\n",
       "  'providerName': 'AI21 Labs',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': False,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-jumbo-instruct',\n",
       "  'modelId': 'ai21.j2-jumbo-instruct',\n",
       "  'modelName': 'J2 Jumbo Instruct',\n",
       "  'providerName': 'AI21 Labs',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': False,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-mid',\n",
       "  'modelId': 'ai21.j2-mid',\n",
       "  'modelName': 'Jurassic-2 Mid',\n",
       "  'providerName': 'AI21 Labs',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': False,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-mid-v1',\n",
       "  'modelId': 'ai21.j2-mid-v1',\n",
       "  'modelName': 'Jurassic-2 Mid',\n",
       "  'providerName': 'AI21 Labs',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': False,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-ultra',\n",
       "  'modelId': 'ai21.j2-ultra',\n",
       "  'modelName': 'Jurassic-2 Ultra',\n",
       "  'providerName': 'AI21 Labs',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': False,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-ultra-v1',\n",
       "  'modelId': 'ai21.j2-ultra-v1',\n",
       "  'modelName': 'Jurassic-2 Ultra',\n",
       "  'providerName': 'AI21 Labs',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': False,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1:2:100k',\n",
       "  'modelId': 'anthropic.claude-instant-v1:2:100k',\n",
       "  'modelName': 'Claude Instant',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1',\n",
       "  'modelId': 'anthropic.claude-instant-v1',\n",
       "  'modelName': 'Claude Instant',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:0:18k',\n",
       "  'modelId': 'anthropic.claude-v2:0:18k',\n",
       "  'modelName': 'Claude',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:0:100k',\n",
       "  'modelId': 'anthropic.claude-v2:0:100k',\n",
       "  'modelName': 'Claude',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:1:18k',\n",
       "  'modelId': 'anthropic.claude-v2:1:18k',\n",
       "  'modelName': 'Claude',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:1:200k',\n",
       "  'modelId': 'anthropic.claude-v2:1:200k',\n",
       "  'modelName': 'Claude',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:1',\n",
       "  'modelId': 'anthropic.claude-v2:1',\n",
       "  'modelName': 'Claude',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2',\n",
       "  'modelId': 'anthropic.claude-v2',\n",
       "  'modelName': 'Claude',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0',\n",
       "  'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
       "  'modelName': 'Claude 3 Sonnet',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0',\n",
       "  'modelId': 'anthropic.claude-3-haiku-20240307-v1:0',\n",
       "  'modelName': 'Claude 3 Haiku',\n",
       "  'providerName': 'Anthropic',\n",
       "  'inputModalities': ['TEXT', 'IMAGE'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-text-v14:7:4k',\n",
       "  'modelId': 'cohere.command-text-v14:7:4k',\n",
       "  'modelName': 'Command',\n",
       "  'providerName': 'Cohere',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': ['FINE_TUNING'],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-text-v14',\n",
       "  'modelId': 'cohere.command-text-v14',\n",
       "  'modelName': 'Command',\n",
       "  'providerName': 'Cohere',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-light-text-v14:7:4k',\n",
       "  'modelId': 'cohere.command-light-text-v14:7:4k',\n",
       "  'modelName': 'Command Light',\n",
       "  'providerName': 'Cohere',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': ['FINE_TUNING'],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-light-text-v14',\n",
       "  'modelId': 'cohere.command-light-text-v14',\n",
       "  'modelName': 'Command Light',\n",
       "  'providerName': 'Cohere',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-english-v3',\n",
       "  'modelId': 'cohere.embed-english-v3',\n",
       "  'modelName': 'Embed English',\n",
       "  'providerName': 'Cohere',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['EMBEDDING'],\n",
       "  'responseStreamingSupported': False,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-multilingual-v3',\n",
       "  'modelId': 'cohere.embed-multilingual-v3',\n",
       "  'modelName': 'Embed Multilingual',\n",
       "  'providerName': 'Cohere',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['EMBEDDING'],\n",
       "  'responseStreamingSupported': False,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-chat-v1:0:4k',\n",
       "  'modelId': 'meta.llama2-13b-chat-v1:0:4k',\n",
       "  'modelName': 'Llama 2 Chat 13B',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['PROVISIONED'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-chat-v1',\n",
       "  'modelId': 'meta.llama2-13b-chat-v1',\n",
       "  'modelName': 'Llama 2 Chat 13B',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-chat-v1:0:4k',\n",
       "  'modelId': 'meta.llama2-70b-chat-v1:0:4k',\n",
       "  'modelName': 'Llama 2 Chat 70B',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': [],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-chat-v1',\n",
       "  'modelId': 'meta.llama2-70b-chat-v1',\n",
       "  'modelName': 'Llama 2 Chat 70B',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-v1:0:4k',\n",
       "  'modelId': 'meta.llama2-13b-v1:0:4k',\n",
       "  'modelName': 'Llama 2 13B',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': ['FINE_TUNING'],\n",
       "  'inferenceTypesSupported': [],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-v1',\n",
       "  'modelId': 'meta.llama2-13b-v1',\n",
       "  'modelName': 'Llama 2 13B',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': [],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-v1:0:4k',\n",
       "  'modelId': 'meta.llama2-70b-v1:0:4k',\n",
       "  'modelName': 'Llama 2 70B',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': ['FINE_TUNING'],\n",
       "  'inferenceTypesSupported': [],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-v1',\n",
       "  'modelId': 'meta.llama2-70b-v1',\n",
       "  'modelName': 'Llama 2 70B',\n",
       "  'providerName': 'Meta',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': [],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/mistral.mistral-7b-instruct-v0:2',\n",
       "  'modelId': 'mistral.mistral-7b-instruct-v0:2',\n",
       "  'modelName': 'Mistral 7B Instruct',\n",
       "  'providerName': 'Mistral AI',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}},\n",
       " {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/mistral.mixtral-8x7b-instruct-v0:1',\n",
       "  'modelId': 'mistral.mixtral-8x7b-instruct-v0:1',\n",
       "  'modelName': 'Mixtral 8x7B Instruct',\n",
       "  'providerName': 'Mistral AI',\n",
       "  'inputModalities': ['TEXT'],\n",
       "  'outputModalities': ['TEXT'],\n",
       "  'responseStreamingSupported': True,\n",
       "  'customizationsSupported': [],\n",
       "  'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "  'modelLifecycle': {'status': 'ACTIVE'}}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock.list_foundation_models()['modelSummaries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7cb6c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For inference\n",
    "bedrock_inference = session.client(\n",
    "    service_name=\"bedrock-runtime\", region_name=session.region_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f103f0-febc-438e-8f83-6fcfcac31621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2ab23e4-9d0d-4455-8caa-d87e86b98725",
   "metadata": {},
   "source": [
    "## Claude 3 Sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "31aeb341-f562-4a3e-9439-dd72b179df2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def send_prompt(prompt_data, temperature=1.0, top_k=250, top_p=0.999, max_token_count=5000):\n",
    "\n",
    "    body = json.dumps(\n",
    "    {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": max_token_count,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        \"messages\": [\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "              {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"\"\"{prompt_data}\"\"\"\n",
    "              }\n",
    "            ]\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    response = bedrock_inference.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "\n",
    "    return response_body[\"content\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "087407f7-8c89-496d-8cd7-6bdc7aabdab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "query = queries[\"WorkloadEvaluation\"]\n",
    "result = send_prompt(f\"\"\"Please explain the SQL command. Explain the numbers appearing in the query. think step by step. <SQLcommand> {query}  </SQLcommand>\"\"\", max_token_count=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "18e276a1-e54b-4a63-957d-7d5002fdf68d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I can explain the SQL command step by step, including the numbers appearing in the query.\n",
      "\n",
      "This is a complex SQL query that involves multiple common table expressions (CTEs) and subqueries. The query aims to analyze the workload of queries based on their scan size and execution time, categorizing them into small, medium, and large workloads.\n",
      "\n",
      "1. `WITH` clause:\n",
      "   - This clause is used to define common table expressions (CTEs), which are temporary result sets that can be referenced later in the query.\n",
      "\n",
      "2. `hour_list` CTE:\n",
      "   ```sql\n",
      "   WITH hour_list AS\n",
      "   (\n",
      "     SELECT DATE_TRUNC('m', starttime) start_hour,\n",
      "            dateadd('m', 1, start_hour) AS end_hour\n",
      "     FROM stl_query q\n",
      "     WHERE starttime >= (getdate() - 7)\n",
      "     GROUP BY 1\n",
      "   )\n",
      "   ```\n",
      "   - This CTE creates a list of hourly intervals for the past 7 days.\n",
      "   - `DATE_TRUNC('m', starttime)` truncates the `starttime` to the nearest minute.\n",
      "   - `dateadd('m', 1, start_hour)` adds 1 minute to the `start_hour` to get the `end_hour`.\n",
      "   - The `GROUP BY 1` groups the results by the first column (`start_hour`).\n",
      "\n",
      "3. `scan_sum` CTE:\n",
      "   ```sql\n",
      "   scan_sum AS\n",
      "   (\n",
      "     SELECT query,\n",
      "            segment,\n",
      "            SUM(bytes) AS bytes\n",
      "     FROM stl_scan\n",
      "     WHERE userid > 1\n",
      "     GROUP BY query,\n",
      "              segment\n",
      "   )\n",
      "   ```\n",
      "   - This CTE calculates the sum of bytes scanned by each query and segment.\n",
      "   - The `WHERE userid > 1` condition filters out system queries.\n",
      "   - The results are grouped by `query` and `segment`.\n",
      "\n",
      "4. `scan_list` CTE:\n",
      "   ```sql\n",
      "   scan_list AS\n",
      "   (\n",
      "     SELECT query,\n",
      "            MAX(bytes) AS max_scan_bytes\n",
      "     FROM scan_sum\n",
      "     GROUP BY query\n",
      "   )\n",
      "   ```\n",
      "   - This CTE retrieves the maximum number of bytes scanned for each query by taking the maximum value from the `scan_sum` CTE.\n",
      "\n",
      "5. `query_list` CTE:\n",
      "   ```sql\n",
      "   query_list AS\n",
      "   (\n",
      "     SELECT w.query,\n",
      "            exec_start_time,\n",
      "            exec_end_time,\n",
      "            ROUND(total_exec_time / 1000 / 1000.0, 3) AS exec_sec,\n",
      "            max_scan_bytes,\n",
      "            CASE\n",
      "              WHEN max_scan_bytes < 100000000 THEN 'small'\n",
      "              WHEN max_scan_bytes BETWEEN 100000000 AND 500000000000 THEN 'medium'\n",
      "              WHEN max_scan_bytes > 500000000000 THEN 'large'\n",
      "            END AS size_type\n",
      "     FROM stl_wlm_query w,\n",
      "          scan_list sc\n",
      "     WHERE sc.query = w.query\n",
      "   )\n",
      "   ```\n",
      "   - This CTE joins the `stl_wlm_query` table with the `scan_list` CTE to get query-level information.\n",
      "   - It calculates the execution time in seconds (`exec_sec`) by converting `total_exec_time` from microseconds to seconds.\n",
      "   - It categorizes the queries into 'small', 'medium', and 'large' based on the `max_scan_bytes` using a `CASE` statement.\n",
      "     - `< 100000000` bytes = 'small'\n",
      "     - `BETWEEN 100000000 AND 500000000000` bytes = 'medium'\n",
      "     - `> 500000000000` bytes = 'large'\n",
      "\n",
      "6. `workload_exec_seconds` CTE:\n",
      "   ```sql\n",
      "   workload_exec_seconds AS\n",
      "   (\n",
      "     SELECT\n",
      "       count(*) as query_cnt,\n",
      "       SUM(CASE WHEN size_type = 'small' THEN exec_sec ELSE 0 END) AS small_workload_exec_sec_sum,\n",
      "       SUM(CASE WHEN size_type = 'medium' THEN exec_sec ELSE 0 END) AS medium_workload_exec_sec_sum,\n",
      "       SUM(CASE WHEN size_type = 'large' THEN exec_sec ELSE 0 END) AS large_workload_exec_sec_sum,\n",
      "       AVG(CASE WHEN size_type = 'small' THEN exec_sec ELSE 0 END) AS small_workload_exec_sec_avg,\n",
      "       AVG(CASE WHEN size_type = 'medium' THEN exec_sec ELSE 0 END) AS medium_workload_exec_sec_avg,\n",
      "       AVG(CASE WHEN size_type = 'large' THEN exec_sec ELSE 0 END) AS large_workload_exec_sec_avg,\n",
      "       MAX(CASE WHEN size_type = 'small' THEN exec_sec ELSE 0 END) AS small_workload_exec_sec_max,\n",
      "       MAX(CASE WHEN size_type = 'medium' THEN exec_sec ELSE 0 END) AS medium_workload_exec_sec_max,\n",
      "       MAX(CASE WHEN size_type = 'large' THEN exec_sec ELSE 0 END) AS large_workload_exec_sec_max,\n",
      "       MIN(CASE WHEN size_type = 'small' THEN exec_sec ELSE 0 END) AS small_workload_exec_sec_min,\n",
      "       MIN(CASE WHEN size_type = 'medium' THEN exec_sec ELSE 0 END) AS medium_workload_exec_sec_min,\n",
      "       MIN(CASE WHEN size_type = 'large' THEN exec_sec ELSE 0 END) AS large_workload_exec_sec_min,\n",
      "       AVG(CASE WHEN size_type = 'small' THEN max_scan_bytes ELSE 0 END) AS small_workload_max_scan_bytes_avg,\n",
      "       AVG(CASE WHEN size_type = 'medium' THEN max_scan_bytes ELSE 0 END) AS medium_workload_max_scan_bytes_avg,\n",
      "       AVG(CASE WHEN size_type = 'large' THEN max_scan_bytes ELSE 0 END) AS large_workload_max_scan_bytes_avg,\n",
      "       (small_workload_exec_sec_sum + medium_workload_exec_sec_sum + large_workload_exec_sec_sum) as total_workload_exec_sec_sum,\n",
      "       small_workload_exec_sec_sum / (total_workload_exec_sec_sum * 1.00) as Small_workload_perc,\n",
      "       medium_workload_exec_sec_sum / (total_workload_exec_sec_sum * 1.00) as Medium_workload_perc,\n",
      "       large_workload_exec_sec_sum / (total_workload_exec_sec_sum * 1.00) as Large_workload_perc\n",
      "     FROM query_list\n",
      "   )\n",
      "   ```\n",
      "   - This CTE calculates various statistics for each workload type (small, medium, and large) based on the `query_list` CTE.\n",
      "   - It calculates the count, sum, average, maximum, and minimum execution times for each workload type using `CASE` statements.\n",
      "   - It also calculates the average maximum scan bytes for each workload type.\n",
      "   - Additionally, it calculates the total execution time for all queries and the percentage contribution of each workload type to the total execution time.\n",
      "\n",
      "The remaining CTEs (`query_list_2`, `hour_list_agg`, `utilization_perc`, `activity_perc`, `mincount`, `avgmincount`, `final_output`) and the final `SELECT` statement combine the results from the previous CTEs to provide a comprehensive analysis of the workload, including query counts, execution times, scan sizes, and activity percentages for each workload type.\n",
      "\n",
      "The numbers appearing in the query, such as `100000000` and `500000000000`, are used as thresholds to categorize queries into small, medium, and large workloads based on the maximum number of bytes scanned.\n",
      "\n",
      "Overall, this query performs a detailed analysis of the workload by categorizing queries based on their scan size, calculating various statistics for each workload type, and providing insights into query counts, execution times, and activity percentages.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9047372-7332-47a1-b923-151e97647113",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = send_prompt(f\"\"\"Please explain the SQL command. Explain the numbers appearing in the query. think step by step. <SQLcommand> {query}  </SQLcommand>\"\"\", max_token_count=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "057ef5ca-3d51-448d-a938-68b3b2921359",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, let's break down the SQL query step by step:\n",
      "\n",
      "1. `WITH hour_list AS (...)`: This is a common table expression (CTE) named `hour_list`. It generates a list of hourly intervals for the past 7 days by truncating the `starttime` from the `stl_query` table to the nearest hour and adding one hour to create the `end_hour`.\n",
      "\n",
      "2. `scan_sum AS (...)`: Another CTE that calculates the sum of bytes scanned for each query and segment from the `stl_scan` table, excluding user IDs less than or equal to 1 (system processes).\n",
      "\n",
      "3. `scan_list AS (...)`: This CTE selects the maximum bytes scanned for each query from the `scan_sum` CTE.\n",
      "\n",
      "4. `query_list AS (...)`: This CTE combines information from `stl_wlm_query` and `scan_list` CTEs. It calculates the execution time, retrieves the maximum scan bytes, and categorizes queries into 'small', 'medium', or 'large' based on the scan bytes.\n",
      "\n",
      "5. `workload_exec_seconds AS (...)`: This CTE aggregates and calculates various metrics related to execution time and scan bytes for each size category ('small', 'medium', 'large'). It includes counts, sums, averages, maximums, and minimums.\n",
      "\n",
      "6. `query_list_2 AS (...)`: This CTE joins the `hour_list` and `query_list` CTEs to associate queries with their corresponding hourly intervals based on their execution start and end times.\n",
      "\n",
      "7. `hour_list_agg AS (...)`: This CTE aggregates the counts of 'small', 'medium', and 'large' queries for each hourly interval from the `query_list_2` CTE.\n",
      "\n",
      "8. `utilization_perc AS (...)`: This CTE calculates the percentage of hourly intervals with any query activity, as well as the percentage of intervals with 'small', 'medium', and 'large' queries.\n",
      "\n",
      "9. `activity_perc AS (...)`: This CTE calculates the average percentage of hourly intervals with 'small', 'medium', and 'large' query activity from the `utilization_perc` CTE.\n",
      "\n",
      "10. `mincount AS (...)`: This CTE counts the number of hourly intervals with any query activity, as well as the number of intervals with 'small', 'medium', and 'large' queries.\n",
      "\n",
      "11. `avgmincount AS (...)`: This CTE calculates the average number of hourly intervals with 'small', 'medium', and 'large' query activity from the `mincount` CTE.\n",
      "\n",
      "12. `final_output AS (...)`: This CTE combines the results from various CTEs to produce a comprehensive summary of the query workload, including percentages, averages, maximums, minimums, and counts for each size category ('small', 'medium', 'large').\n",
      "\n",
      "Finally, the outer query selects and rearranges the desired columns from the `final_output` CTE, presenting the results for each workload type ('Small', 'Medium', 'Large') with corresponding metrics.\n",
      "\n",
      "The numbers appearing in the query are primarily used for grouping, filtering, or case statements to handle different scenarios or categories. For example, the number `1` in the `WHERE userid > 1` clause filters out system processes, and the numbers `100000000` and `500000000000` are used as thresholds to categorize queries as 'small', 'medium', or 'large' based on the scan bytes.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8f576f44-dc7f-42be-85f4-d01fc6a6cb98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "731.2"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7e9f72ad-7a1f-47bd-9524-6111f717de1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = send_prompt(f\"\"\"Please explain the SQL command. Explain the numbers appearing in the query. <SQLcommand> {query}  </SQLcommand>\"\"\", max_token_count=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f1a55fff-a0b4-49cc-8140-f5cd06e75140",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This SQL query is quite complex and involves several nested subqueries and Common Table Expressions (CTEs). Let me break it down and explain each part:\n",
      "\n",
      "1. **WITH hour_list AS**: This is a CTE that generates a list of hourly intervals for the past 7 days. It truncates the time to the start of the hour and calculates the end of the hour by adding 1 minute. This is used later to associate queries with the hourly intervals they ran in.\n",
      "\n",
      "2. **scan_sum AS**: This CTE sums up the bytes scanned by each query and segment from the `stl_scan` table, grouping by query and segment. This is used to calculate the maximum scan bytes for each query.\n",
      "\n",
      "3. **scan_list AS**: This CTE finds the maximum scan bytes for each query by taking the maximum value from the `scan_sum` CTE.\n",
      "\n",
      "4. **query_list AS**: This CTE joins the `stl_wlm_query` table with the `scan_list` CTE to get information about each query, including start and end times, execution time, maximum scan bytes, and a size classification based on the maximum scan bytes (small, medium, or large).\n",
      "\n",
      "5. **workload_exec_seconds AS**: This CTE calculates various aggregate metrics for the queries, such as the count of queries, sum of execution times, average execution times, maximum execution times, and minimum execution times, broken down by the size classification (small, medium, or large). It also calculates the percentage of the total workload represented by each size classification.\n",
      "\n",
      "6. **query_list_2 AS**: This CTE associates each query with the hourly intervals from the `hour_list` CTE based on the query's start and end times.\n",
      "\n",
      "7. **hour_list_agg AS**: This CTE counts the number of small, medium, and large queries in each hourly interval.\n",
      "\n",
      "8. **utilization_perc AS**: This CTE calculates the percentage of time that small, medium, and large queries were running in each hourly interval.\n",
      "\n",
      "9. **activity_perc AS**: This CTE calculates the average percentage of time that small, medium, and large queries were running across all hourly intervals.\n",
      "\n",
      "10. **mincount AS**: This CTE counts the number of minutes in each hourly interval where small, medium, and large queries were running.\n",
      "\n",
      "11. **avgmincount AS**: This CTE calculates the average number of minutes across all hourly intervals where small, medium, and large queries were running.\n",
      "\n",
      "12. **final_output AS**: This CTE combines the results from several previous CTEs to produce a single output with various metrics related to the workload and activity of small, medium, and large queries.\n",
      "\n",
      "13. The final `SELECT` statement pivots the data from the `final_output` CTE to produce separate rows for small, medium, and large queries, with columns showing the percentage of total workload, percentage of duration in a day, total query minutes in a day, average/minimum/maximum execution times, query count, and average scan bytes.\n",
      "\n",
      "The numbers appearing in the query generally represent literal values, such as:\n",
      "\n",
      "- `1`: Used in boolean expressions or `CASE` statements to represent `true`.\n",
      "- `100000000`: A byte value used to classify queries as small, medium, or large based on their maximum scan bytes.\n",
      "- `500000000000`: Another byte value used to classify queries as small, medium, or large based on their maximum scan bytes.\n",
      "- `1440.0`: The number of minutes in a day, used to calculate the percentage of time that queries were running.\n",
      "\n",
      "Overall, this query appears to be analyzing the workload and activity of queries in a database system, with a focus on classifying queries based on their maximum scan bytes and calculating various metrics related to their execution times and activity patterns over hourly intervals in the past 7 days.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243abc31-2e06-4673-bbf3-7977054cab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "query = queries[\"WorkloadEvaluation\"]\n",
    "result = send_prompt(f\"\"\"Please explain the SQL command. Explain the numbers appearing in the query.<SQLcommand> {query}  </SQLcommand>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1dced80e-d74b-44bd-a693-761a66f73130",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided SQL query is quite complex and consists of multiple sub-queries (common table expressions or CTEs) and a final SELECT statement. Let me break it down for you:\n",
      "\n",
      "1. `WITH hour_list AS (...)`: This CTE generates a list of hourly time intervals for the last 7 days by truncating the `starttime` column from the `stl_query` table to the nearest hour and then adding 1 hour to create the `end_hour`.\n",
      "\n",
      "2. `scan_sum AS (...)`: This CTE calculates the sum of bytes scanned for each query and segment from the `stl_scan` table, excluding user IDs less than or equal to 1.\n",
      "\n",
      "3. `scan_list AS (...)`: This CTE finds the maximum bytes scanned for each query by grouping the results from the `scan_sum` CTE.\n",
      "\n",
      "4. `query_list AS (...)`: This CTE joins the `stl_wlm_query` table with the `scan_list` CTE to retrieve various query-related metrics, such as execution times, maximum bytes scanned, and a size classification based on the maximum bytes scanned.\n",
      "\n",
      "5. `workload_exec_seconds AS (...)`: This CTE calculates various aggregate metrics related to query execution times, such as counts, sums, averages, maximums, and minimums, grouped by the size classification ('small', 'medium', or 'large').\n",
      "\n",
      "6. `query_list_2 AS (...)`: This CTE joins the `hour_list` and `query_list` CTEs to associate each query with the hourly time interval it falls within.\n",
      "\n",
      "7. `hour_list_agg AS (...)`: This CTE aggregates query counts by hourly time interval and size classification.\n",
      "\n",
      "8. `utilization_perc AS (...)`: This CTE calculates the percentage of minutes in each day where queries of different size classifications were active.\n",
      "\n",
      "9. `activity_perc AS (...)`: This CTE computes the average percentage of minutes in a day where queries of different size classifications were active.\n",
      "\n",
      "10. `mincount AS (...)`: This CTE counts the number of minutes in each day where queries of different size classifications were active.\n",
      "\n",
      "11. `avgmincount AS (...)`: This CTE calculates the average number of minutes in a day where queries of different size classifications were active.\n",
      "\n",
      "12. `final_output AS (...)`: This CTE combines the results from various previous CTEs to generate a consolidated output.\n",
      "\n",
      "Finally, the main SELECT statement extracts and formats the desired output columns from the `final_output` CTE, using a UNION to combine results for different workload types ('Small', 'Medium', and 'Large').\n",
      "\n",
      "The numbers appearing in the query are mostly related to date/time calculations, case statements for conditional aggregations, and ordering of the final output.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f4e8b-7c7b-4040-bdcb-0de89f638017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "query = queries[\"WorkloadEvaluation\"]\n",
    "result = send_prompt(f\"\"\"Please explain the SQL command. <SQLcommand> {query}  </SQLcommand>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d41fb3fd-04d7-49be-9738-44b2dcac6bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided SQL command is a complex query that performs several operations to analyze the workload and performance of queries in a database system, specifically in a Redshift environment.\n",
      "\n",
      "Here's a breakdown of the different parts of the query:\n",
      "\n",
      "1. **CTE (Common Table Expressions):**\n",
      "   - `hour_list`: This CTE generates a list of hourly intervals by truncating the `starttime` column from the `stl_query` table to the nearest minute and adding one minute to get the end of the hour range. It groups the data by the truncated `starttime` and considers only queries within the last 7 days.\n",
      "   - `scan_sum`: This CTE calculates the sum of bytes scanned for each query and segment combination from the `stl_scan` table, excluding user ID 1 (superuser).\n",
      "   - `scan_list`: This CTE finds the maximum bytes scanned for each query by grouping the results from `scan_sum`.\n",
      "   - `query_list`: This CTE joins the `stl_wlm_query` table with `scan_list` to get the execution time, maximum scan bytes, and a size classification (small, medium, or large) based on the maximum scan bytes.\n",
      "   - `workload_exec_seconds`: This CTE calculates various aggregations and metrics related to the query execution time and size classification, such as the sum, average, maximum, and minimum execution times, as well as the percentage of workload for each size type.\n",
      "   - `query_list_2`: This CTE joins the `hour_list` and `query_list` CTEs to associate each query with the hourly interval it belongs to.\n",
      "   - `hour_list_agg`: This CTE aggregates the query count for each size type (small, medium, large) and the total query count per hourly interval.\n",
      "   - `utilization_perc`: This CTE calculates the percentage of hourly intervals that had active queries for each size type and overall.\n",
      "   - `activity_perc`: This CTE calculates the average percentage of hourly intervals that had active queries for each size type.\n",
      "   - `mincount`: This CTE counts the number of minutes that had active queries for each size type and overall.\n",
      "   - `avgmincount`: This CTE calculates the average number of minutes that had active queries for each size type.\n",
      "   - `final_output`: This CTE combines the results from various CTEs to create a final output table.\n",
      "\n",
      "2. **Final Query:**\n",
      "   The final query selects the desired columns from the `final_output` CTE, including the percentage of total workload, percentage of duration in a day, total query minutes in a day, average/minimum/maximum execution time, query count, and average scan bytes. It unions the results for small, medium, and large workload types and orders them by the `id` column.\n",
      "\n",
      "The query aims to provide a comprehensive analysis of the workload in the database system, including the distribution of queries based on their size (determined by the maximum scan bytes), their execution times, and their activity patterns over time (hourly intervals and minutes). This information can be useful for identifying performance bottlenecks, optimizing queries, and managing resource allocation.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c428737b-1e20-4018-8c3b-f9d6020b18b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = send_prompt(f\"\"\"Please explain the SQL command. Explain the numbers appearing in the query. Identify all system tables used and present this as CSV using <tables> table1, table2, ... </tables>   <SQLcommand> {query}  </SQLcommand>\"\"\", max_token_count=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cfe435c2-a6ba-4b64-bbf7-181b1d574882",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided SQL command is a complex query that performs various calculations and aggregations to analyze and report on the workload and query activity in a database system, specifically for different query size types (small, medium, and large) based on their maximum scan bytes.\n",
      "\n",
      "Explanation of the numbers appearing in the query:\n",
      "\n",
      "1. The number 1 in `WHERE userid > 1` filters out system-generated queries and considers only user queries.\n",
      "2. The numbers 100000000 and 500000000000 are used as thresholds to categorize queries as 'small', 'medium', or 'large' based on their maximum scan bytes.\n",
      "3. The number 7 in `WHERE starttime >= (getdate() - 7)` selects queries executed within the last 7 days.\n",
      "4. The numbers 1000 and 1000000.0 are used for converting execution times from microseconds to seconds.\n",
      "5. The number 1440.0 represents the number of minutes in a day (60 minutes  24 hours) and is used for calculating the percentage of query activity during the day.\n",
      "\n",
      "The system tables used in the query are:\n",
      "\n",
      "<tables>\n",
      "stl_query,\n",
      "stl_scan,\n",
      "stl_wlm_query\n",
      "</tables>\n",
      "\n",
      "These tables are likely system tables in a database system that store information about executed queries, their scans, and workload management details.\n",
      "\n",
      "The query is presented as a CSV as requested:\n",
      "\n",
      "workloadtype,perc_of_total_workload,perc_duration_in_day,Total_query_minutes_in_day,workload_exec_sec_avg,workload_exec_sec_min,workload_exec_sec_max,query_cnt,scan_bytes_avg\n",
      "Small,24.8,14.4,207.8,0.004,0.0,0.081,5781,14896833.0\n",
      "Meduim,70.0,41.6,598.8,0.116,0.0,2.634,10459,196855395.0\n",
      "Large,5.2,3.1,44.9,1.186,0.0,48.615,743,2508525408.0\n",
      "\n",
      "This output summarizes the workload characteristics for small, medium, and large queries, including their percentage of total workload, percentage of duration in a day, total query minutes in a day, average execution time, minimum and maximum execution times, query count, and average maximum scan bytes.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a589d4a9-9ce0-42e5-b0bc-f30b651bd6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e11351-6680-4ef6-94e3-981cbaefc6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7de8020-69dc-49dc-b3e9-3fcde3a7a863",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please explain the SQL command <SQLcommand> hohoh  </SQLcommand>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\"Please explain the SQL command <SQLcommand> {query}  </SQLcommand>\"\"\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbf5e1-d0b5-4e31-a2e6-d87cd8b7e148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ff7927-3a44-4033-bd81-73111085f92c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de36b46-eb71-4cf9-8e0d-3c4b83c7b77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8887dce-5c79-4bff-a084-a5863fa7ac80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4ad514-da50-41ff-ba2c-349e00059ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca1549b5-d84d-4b44-9dd2-9ce87afaba96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters=\"\"\"Randomness and diversity\n",
    "Info\n",
    "Temperature\n",
    "1\n",
    "Top P\n",
    "0.999\n",
    "Top K\n",
    "250\n",
    "Length\n",
    "Info\n",
    "Maximum Length\n",
    "2000\n",
    "Stop sequences\n",
    "Add\n",
    "Human:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41d109c8-915a-438b-a0bd-d03c06e1460c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Please explain the SQL command <SQLcommand> SELECT IQ.*,\n",
    "       (IQ.wlm_queue_time_ms/ IQ.wlm_start_commit_time_ms)*100.0::numeric(6,2) AS pct_wlm_queue_time,\n",
    "       (IQ.exec_only_time_ms/ IQ.wlm_start_commit_time_ms)*100.0::numeric(6,2) AS pct_exec_only_time,\n",
    "       (IQ.commit_queue_time_ms/ IQ.wlm_start_commit_time_ms)*100.0::numeric(6,2) pct_commit_queue_time,\n",
    "       (IQ.commit_time_ms/ IQ.wlm_start_commit_time_ms)*100.0::numeric(6,2) pct_commit_time\n",
    "FROM (SELECT TRUNC(b.starttime) AS DAY,\n",
    "             d.service_class,\n",
    "             rtrim(s.name) as queue_name,\n",
    "             c.node,\n",
    "             COUNT(DISTINCT c.xid) AS count_commit_xid,\n",
    "             SUM(datediff ('microsec',d.service_class_start_time,c.endtime)*0.001)::numeric(38,4) AS wlm_start_commit_time_ms,\n",
    "             SUM(datediff ('microsec',d.queue_start_time,d.queue_end_time)*0.001)::numeric(38,4) AS wlm_queue_time_ms,\n",
    "             SUM(datediff ('microsec',b.starttime,b.endtime)*0.001)::numeric(38,4) AS exec_only_time_ms,\n",
    "             SUM(datediff ('microsec',c.startwork,c.endtime)*0.001)::numeric(38,4) commit_time_ms,\n",
    "             SUM(datediff ('microsec',DECODE(c.startqueue,'2000-01-01 00:00:00',c.startwork,c.startqueue),c.startwork)*0.001)::numeric(38,4) commit_queue_time_ms\n",
    "      FROM stl_query b,\n",
    "           stl_commit_stats c,\n",
    "           stl_wlm_query d,\n",
    "           stv_wlm_service_class_config s\n",
    "      WHERE b.xid = c.xid\n",
    "      AND   b.query = d.query\n",
    "      AND   c.xid > 0\n",
    "      AND d.service_class = s.service_class\n",
    "      GROUP BY 1,2,3,4\n",
    "      ORDER BY 1,2,3,4) IQ;  </SQLcommand>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ce8effe-67f3-483e-b425-f9d8fdf27fc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer=\"\"\"The provided SQL statement is a complex query that performs several calculations and aggregations on data from different system tables in Amazon Redshift. Here's a breakdown of what the query does:\n",
    "\n",
    "1. The subquery (inside the `FROM` clause) retrieves data from multiple system tables: `stl_query`, `stl_commit_stats`, `stl_wlm_query`, and `stv_wlm_service_class_config`. It performs the following operations:\n",
    "   - Groups the data by `DAY` (truncated start time), `service_class`, `queue_name`, and `node`.\n",
    "   - Counts the distinct `xid` values (transaction IDs) for committed queries.\n",
    "   - Calculates the total time (in milliseconds) for various phases of query execution:\n",
    "     - `wlm_start_commit_time_ms`: Time from service class start to query commit.\n",
    "     - `wlm_queue_time_ms`: Time spent in the queue before execution.\n",
    "     - `exec_only_time_ms`: Actual execution time of the query.\n",
    "     - `commit_time_ms`: Time taken for the commit phase.\n",
    "     - `commit_queue_time_ms`: Time spent in the commit queue before the commit phase.\n",
    "\n",
    "2. The outer query selects all columns from the subquery (`IQ.*`) and calculates the following additional columns:\n",
    "   - `pct_wlm_queue_time`: Percentage of time spent in the queue before execution, relative to the total time from service class start to commit.\n",
    "   - `pct_exec_only_time`: Percentage of actual execution time relative to the total time from service class start to commit.\n",
    "   - `pct_commit_queue_time`: Percentage of time spent in the commit queue before the commit phase, relative to the total time from service class start to commit.\n",
    "   - `pct_commit_time`: Percentage of time spent in the commit phase, relative to the total time from service class start to commit.\n",
    "\n",
    "The purpose of this query is to analyze the performance of queries executed in Amazon Redshift, particularly the time spent in various phases of execution, such as queuing, execution, and commit. The calculated percentage columns provide insights into the relative distribution of time across these phases, which can help identify potential bottlenecks or areas for optimization.\n",
    "\n",
    "It's important to note that this query retrieves data from several system tables in Amazon Redshift, which are designed to provide detailed information about query execution, workload management, and system performance. The specific tables used in this query are:\n",
    "\n",
    "- `stl_query`: Contains information about executed queries, including start and end times, query text, and other query-level metrics.\n",
    "- `stl_commit_stats`: Provides commit-related statistics for queries, such as commit time and queue time.\n",
    "- `stl_wlm_query`: Stores workload management-related data for queries, including service class information and queue times.\n",
    "- `stv_wlm_service_class_config`: Contains configuration details for the defined service classes in the system.\n",
    "\n",
    "By combining data from these tables, the query can provide a comprehensive view of query performance and resource utilization in Amazon Redshift.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37961383-9938-413a-b127-bf82f2e8dea5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_log={\"model\":model,\"prompt\":prompt,\"answer\":answer,\"parameters\":parameters,\"query\":}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee2208f-deb6-4066-b7ae-fd325ab0d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59367663-3d09-42b8-b579-21da6c518519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b67b39-501e-4ec5-bbfd-4f43d8fd5847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cc4b8bfd-ea41-4b25-9066-d5a02c7881cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table1=\"\"\"Column name  |  Data type  |  Description   \n",
    "---|---|---  \n",
    "event  |  character(50)  |  Connection or authentication event.   \n",
    "record_time  |  timestamp  |  Time the event occurred.   \n",
    "remote_host  |  character(45)  |  Name or IP address of remote host.   \n",
    "remote_port  |  character(32)  |  Port number for remote host.   \n",
    "session_id  |  integer  |  Process ID associated with the statement.   \n",
    "database_name  |  character(50)  |  Database name.   \n",
    "user_name  |  character(50)  |  Username.   \n",
    "auth_method  |  character(32)  |  Authentication method.   \n",
    "duration  |  integer  |  Duration of connection in microseconds.   \n",
    "ssl_version  |  character(50)  |  Secure Sockets Layer (SSL) version.   \n",
    "ssl_cipher  |  character(128)  |  SSL cipher.   \n",
    "mtu  |  integer  |  Maximum transmission unit (MTU).   \n",
    "ssl_compression  |  character(64)  |  SSL compression type.   \n",
    "ssl_expansion  |  character(64)  |  SSL expansion type.   \n",
    "iam_auth_guid  |  character(36)  |  The IAM authentication ID for the CloudTrail request.   \n",
    "application_name  |  character(250)  |  The initial or updated name of the application for a session.   \n",
    "driver_version  |  character(64)  |  The version of ODBC or JDBC driver that connects to your Amazon Redshift cluster from your third-party SQL client tools.   \n",
    "os_version  |  character(64)  |  The version of the operating system that is on the client machine that connects to your Amazon Redshift cluster.   \n",
    "plugin_name  |  character(32)  |  The name of the plugin used to connect to your Amazon Redshift cluster.   \n",
    "protocol_version  |  integer  |  The internal protocol version that the Amazon Redshift driver uses when establishing its connection with the server. The protocol versions are negotiated between the driver and server. The version describes the features available. Valid values include: 0 (BASE_SERVER_PROTOCOL_VERSION) 1 (EXTENDED_RESULT_METADATA_SERVER_PROTOCOL_VERSION)  To save a round trip per query, the server sends extra result set metadata information. 2 (BINARY_PROTOCOL_VERSION)  Depending on the data type of the result set, the server sends data in binary format. 3 (EXTENDED2_RESULT_METADATA_SERVER_PROTOCOL_VERSION)  The server sends case sensitivity (collation) information of a column.   \n",
    "global_session_id  |  character(36)  |  The globally unique identifier for the current session. The session ID persists through node failure restarts.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fb7f7265-8cdf-4464-92c5-4c220182a1ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table=\"\"\"Column name  |  Data type  |  Description   \n",
    "---|---|---  \n",
    "share_id  |  integer  |  The object ID (OID) of the datashare.   \n",
    "share_name  |  varchar(128)  |  The name of the datashare.   \n",
    "request_id  |  varchar(50)  |  The unique ID of the requested API call.   \n",
    "request_type  |  varchar(25)  |  The type of the request made to the producer cluster.   \n",
    "object_type  |  varchar(64)  |  The type of the object being shared from the datashare. Possible values are schemas, tables, columns, functions, and views.   \n",
    "object_oid  |  integer  |  The ID of the object being shared from the datashare.   \n",
    "object_name  |  varchar(128)  |  The name of the object being shared from the datashare.   \n",
    "consumer_account  |  varchar(16)  |  The account of the consumer account that the datashare is shared to.   \n",
    "consumer_namespace  |  varchar(64)  |  The namespace of the consumer account that the datashare is shared to.   \n",
    "consumer_transaction_uid  |  varchar(50)  |  The unique transaction ID of the statement on the consumer cluster.   \n",
    "record_time  |  timestamp  |  The time when the action is recorded.   \n",
    "status  |  integer  |  The status of the datashare.   \n",
    "error_message  |  varchar(512)  |  The message for an error.   \n",
    "consumer_region  |  char(64)  |  The Region that the consumer cluster is in. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5a7a73ea-f296-47ea-9d23-b097ff9aa7d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "result = send_prompt(f\"\"\"Please write 10 informative troubleshooting SQL queries on the table described in markdown.<markdown> {table1}  </markdown>\"\"\", max_token_count=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b58c08eb-35a4-44ba-b438-d108c0cfae6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. `SELECT object_name, object_type FROM datashare_table WHERE object_type = 'tables';`\n",
      "   This query retrieves the names and types of all objects that are tables from the datashare.\n",
      "\n",
      "2. `SELECT DISTINCT consumer_account, consumer_namespace FROM datashare_table;`\n",
      "   This query retrieves the distinct consumer accounts and namespaces that the datashares are shared with.\n",
      "\n",
      "3. `SELECT share_name, COUNT(DISTINCT object_name) AS num_objects FROM datashare_table GROUP BY share_name;`\n",
      "   This query retrieves the names of the datashares and the number of distinct objects shared in each datashare.\n",
      "\n",
      "4. `SELECT request_type, COUNT(*) AS request_count FROM datashare_table GROUP BY request_type;`\n",
      "   This query retrieves the different types of requests made to the producer cluster and the count of each request type.\n",
      "\n",
      "5. `SELECT consumer_region, COUNT(*) AS region_count FROM datashare_table GROUP BY consumer_region;`\n",
      "   This query retrieves the distinct consumer regions and the count of datashares shared to each region.\n",
      "\n",
      "6. `SELECT status, COUNT(*) AS status_count FROM datashare_table GROUP BY status;`\n",
      "   This query retrieves the distinct status codes and the count of datashares for each status.\n",
      "\n",
      "7. `SELECT share_name, object_type, COUNT(*) AS object_count FROM datashare_table GROUP BY share_name, object_type;`\n",
      "   This query retrieves the names of the datashares, the types of objects shared, and the count of each object type in each datashare.\n",
      "\n",
      "8. `SELECT share_name, MAX(record_time) AS latest_record_time FROM datashare_table GROUP BY share_name;`\n",
      "   This query retrieves the names of the datashares and the latest record time for each datashare.\n",
      "\n",
      "9. `SELECT consumer_transaction_uid, object_name, object_type FROM datashare_table WHERE consumer_transaction_uid IS NOT NULL;`\n",
      "   This query retrieves the consumer transaction UIDs, object names, and object types for datashares where the consumer transaction UID is not null.\n",
      "\n",
      "10. `SELECT share_name, error_message FROM datashare_table WHERE error_message IS NOT NULL;`\n",
      "    This query retrieves the names of the datashares and the error messages for datashares where an error message is present.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1412c94a-f1d9-4a2f-b463-0f1adc507581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "result = send_prompt(f\"\"\"Please write 10 informative troubleshooting SQL queries on the table described in markdown.<markdown> {table1}  </markdown>\"\"\", max_token_count=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "04d5567f-ad93-43e8-907d-b7c566fe6e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 informative troubleshooting SQL queries on the given table:\n",
      "\n",
      "1. **Identify slow connections**\n",
      "```sql\n",
      "SELECT remote_host, duration, session_id\n",
      "FROM table_name\n",
      "ORDER BY duration DESC\n",
      "LIMIT 10;\n",
      "```\n",
      "This query retrieves the top 10 longest connections based on the `duration` column, along with the `remote_host` and `session_id`. It can help identify slow connections and potential performance issues.\n",
      "\n",
      "2. **Check for failed authentication attempts**\n",
      "```sql\n",
      "SELECT remote_host, user_name, auth_method, event, record_time\n",
      "FROM table_name\n",
      "WHERE event LIKE '%failed%'\n",
      "ORDER BY record_time DESC\n",
      "LIMIT 20;\n",
      "```\n",
      "This query retrieves the most recent 20 records where the `event` column contains the word \"failed\". It can help identify potential security issues or brute-force attacks.\n",
      "\n",
      "3. **Monitor connections by user**\n",
      "```sql\n",
      "SELECT user_name, COUNT(*) AS connection_count\n",
      "FROM table_name\n",
      "GROUP BY user_name\n",
      "ORDER BY connection_count DESC;\n",
      "```\n",
      "This query counts the number of connections per user and orders the results by the connection count in descending order. It can help identify users with an unusually high number of connections.\n",
      "\n",
      "4. **Analyze connections by remote host**\n",
      "```sql\n",
      "SELECT remote_host, COUNT(*) AS connection_count\n",
      "FROM table_name\n",
      "GROUP BY remote_host\n",
      "ORDER BY connection_count DESC;\n",
      "```\n",
      "This query counts the number of connections per remote host and orders the results by the connection count in descending order. It can help identify potential issues with specific remote hosts or IP addresses.\n",
      "\n",
      "5. **Check for SSL-related issues**\n",
      "```sql\n",
      "SELECT ssl_version, ssl_cipher, ssl_compression, ssl_expansion, COUNT(*) AS count\n",
      "FROM table_name\n",
      "GROUP BY ssl_version, ssl_cipher, ssl_compression, ssl_expansion\n",
      "ORDER BY count DESC;\n",
      "```\n",
      "This query groups the records by SSL-related columns (`ssl_version`, `ssl_cipher`, `ssl_compression`, `ssl_expansion`) and counts the occurrences of each combination. It can help identify potential issues related to SSL configurations.\n",
      "\n",
      "6. **Monitor connections by application**\n",
      "```sql\n",
      "SELECT application_name, COUNT(*) AS connection_count\n",
      "FROM table_name\n",
      "GROUP BY application_name\n",
      "ORDER BY connection_count DESC;\n",
      "```\n",
      "This query counts the number of connections per application name and orders the results by the connection count in descending order. It can help identify applications with a high connection load or potential issues with specific applications.\n",
      "\n",
      "7. **Analyze connections by database**\n",
      "```sql\n",
      "SELECT database_name, COUNT(*) AS connection_count\n",
      "FROM table_name\n",
      "GROUP BY database_name\n",
      "ORDER BY connection_count DESC;\n",
      "```\n",
      "This query counts the number of connections per database and orders the results by the connection count in descending order. It can help identify databases with a high connection load or potential issues with specific databases.\n",
      "\n",
      "8. **Check for connections with specific MTU values**\n",
      "```sql\n",
      "SELECT mtu, COUNT(*) AS connection_count\n",
      "FROM table_name\n",
      "GROUP BY mtu\n",
      "ORDER BY connection_count DESC;\n",
      "```\n",
      "This query groups the records by the `mtu` column and counts the occurrences of each MTU value. It can help identify potential issues related to MTU settings or network configurations.\n",
      "\n",
      "9. **Monitor authentication methods**\n",
      "```sql\n",
      "SELECT auth_method, COUNT(*) AS connection_count\n",
      "FROM table_name\n",
      "GROUP BY auth_method\n",
      "ORDER BY connection_count DESC;\n",
      "```\n",
      "This query counts the number of connections per authentication method and orders the results by the connection count in descending order. It can help identify potential issues or security concerns related to specific authentication methods.\n",
      "\n",
      "10. **Analyze connections by client software versions**\n",
      "```sql\n",
      "SELECT driver_version, os_version, COUNT(*) AS connection_count\n",
      "FROM table_name\n",
      "GROUP BY driver_version, os_version\n",
      "ORDER BY connection_count DESC;\n",
      "```\n",
      "This query groups the records by the `driver_version` and `os_version` columns and counts the occurrences of each combination. It can help identify potential issues related to specific client software versions or operating systems.\n",
      "\n",
      "These queries cover various aspects of troubleshooting and monitoring connections, authentication, performance, SSL configurations, and more. They can be modified or combined based on your specific needs and requirements.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9ec49ac0-96c0-4913-8c79-33123b013095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table3=\"\"\"Column name  |  Data type  |  Description   \n",
    "---|---|---  \n",
    "slice  |  integer  |  Node slice.   \n",
    "col  |  integer  |  Zero-based index for the column. Every table you create has three hidden columns appended to it: INSERT_XID, DELETE_XID, and ROW_ID (OID). A table with 3 user-defined columns contains 6 actual columns, and the user-defined columns are internally numbered as 0, 1, and 2. The INSERT_XID, DELETE_XID, and ROW_ID columns are numbered 3, 4, and 5, respectively, in this example.   \n",
    "tbl  |  integer  |  Table ID for the database table.   \n",
    "blocknum  |  integer  |  ID for the data block.   \n",
    "num_values  |  integer  |  Number of values contained on the block.   \n",
    "extended_limits  |  integer  |  For internal use.   \n",
    "minvalue  |  bigint  |  Minimum data value of the block. Stores first eight characters as 64-bit integer for non-numeric data. Used for disk scanning.   \n",
    "maxvalue  |  bigint  |  Maximum data value of the block. Stores first eight characters as 64-bit integer for non-numeric data. Used for disk scanning.   \n",
    "sb_pos  |  integer  |  Internal Amazon Redshift identifier for super block position on the disk.   \n",
    "pinned  |  integer  |  Whether or not the block is pinned into memory as part of pre-load. 0 = false; 1 = true. Default is false.   \n",
    "on_disk  |  integer  |  Whether or not the block is automatically stored on disk. 0 = false; 1 = true. Default is false.   \n",
    "modified  |  integer  |  Whether or not the block has been modified. 0 = false; 1 = true. Default is false.   \n",
    "hdr_modified  |  integer  |  Whether or not the block header has been modified. 0 = false; 1 = true. Default is false.   \n",
    "unsorted  |  integer  |  Whether or not a block is unsorted. 0 = false; 1 = true. Default is true.   \n",
    "tombstone  |  integer  |  For internal use.   \n",
    "preferred_diskno  |  integer  |  Disk number that the block should be on, unless the disk has failed. Once the disk has been fixed, the block will move back to this disk.   \n",
    "temporary  |  integer  |  Whether or not the block contains temporary data, such as from a temporary table or intermediate query results. 0 = false; 1 = true. Default is false.   \n",
    "newblock  |  integer  |  Indicates whether or not a block is new (true) or was never committed to disk (false). 0 = false; 1 = true.   \n",
    "num_readers  |  integer  |  Number of references on each block.   \n",
    "flags  |  integer  |  Internal Amazon Redshift flags for the block header. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2e5b3605-778a-4199-b9c4-f5780a11ab40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "result = send_prompt(f\"\"\"Please write 10 informative troubleshooting SQL queries on the table described in markdown.<markdown> {table3}  </markdown>\"\"\", max_token_count=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f245d07c-65de-4ad1-8470-89a7ee89bf66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 informative troubleshooting SQL queries on the described table:\n",
      "\n",
      "1. **Find blocks with minimum or maximum values outside expected range**\n",
      "```sql\n",
      "SELECT slice, col, tbl, blocknum, minvalue, maxvalue\n",
      "FROM svv_diskusage\n",
      "WHERE minvalue < expected_min_value OR maxvalue > expected_max_value;\n",
      "```\n",
      "\n",
      "2. **Identify pinned blocks**\n",
      "```sql\n",
      "SELECT slice, col, tbl, blocknum\n",
      "FROM svv_diskusage\n",
      "WHERE pinned = 1;\n",
      "```\n",
      "\n",
      "3. **Find modified blocks**\n",
      "```sql\n",
      "SELECT slice, col, tbl, blocknum \n",
      "FROM svv_diskusage\n",
      "WHERE modified = 1;\n",
      "```\n",
      "\n",
      "4. **Check for unsorted blocks**\n",
      "```sql\n",
      "SELECT slice, col, tbl, blocknum\n",
      "FROM svv_diskusage  \n",
      "WHERE unsorted = 1;\n",
      "```\n",
      "\n",
      "5. **Look for temporary blocks**\n",
      "```sql\n",
      "SELECT slice, col, tbl, blocknum\n",
      "FROM svv_diskusage\n",
      "WHERE temporary = 1;\n",
      "```\n",
      "\n",
      "6. **Identify new blocks not committed to disk**\n",
      "```sql\n",
      "SELECT slice, col, tbl, blocknum  \n",
      "FROM svv_diskusage\n",
      "WHERE newblock = 1;\n",
      "```\n",
      "\n",
      "7. **Find blocks with high number of readers**\n",
      "```sql\n",
      "SELECT slice, col, tbl, blocknum, num_readers\n",
      "FROM svv_diskusage\n",
      "WHERE num_readers > 10; -- Adjust threshold as needed\n",
      "```\n",
      "\n",
      "8. **Check for blocks on failed disks**\n",
      "```sql\n",
      "SELECT d.slice, d.col, d.tbl, d.blocknum, d.preferred_diskno, s.failed \n",
      "FROM svv_diskusage d\n",
      "JOIN svv_diskinsampliance s ON d.preferred_diskno = s.diskno\n",
      "WHERE s.failed = 1;\n",
      "```\n",
      "\n",
      "9. **Look for blocks with non-zero tombstone value**\n",
      "```sql\n",
      "SELECT slice, col, tbl, blocknum, tombstone\n",
      "FROM svv_diskusage\n",
      "WHERE tombstone <> 0;\n",
      "```\n",
      "\n",
      "10. **Find blocks with specific internal flags set**\n",
      "```sql\n",
      "SELECT slice, col, tbl, blocknum, flags\n",
      "FROM svv_diskusage\n",
      "WHERE flags = flag_value; -- Replace flag_value with desired flag\n",
      "```\n",
      "\n",
      "These queries can help troubleshoot and identify potential issues related to block status, sorting, pinning, disk failures, and other factors that may impact performance or data integrity.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bd130ad9-4a73-45ee-bf89-a5f94cabf066",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To check if the table data is distributed over all slices, you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT slice, COUNT(DISTINCT tbl) AS num_tables\n",
      "FROM stv_blocklist\n",
      "GROUP BY slice\n",
      "ORDER BY slice;\n",
      "```\n",
      "\n",
      "This query will return the number of distinct tables present on each slice. If the data is distributed across all slices, you should see one row for each slice with a non-zero value in the `num_tables` column.\n",
      "\n",
      "To confirm that the data is truly distributed evenly across all slices, you can compare the `num_tables` values across all slices. If they are relatively similar, it indicates that the data is distributed evenly. However, if you see significant differences in the `num_tables` values, it may suggest an uneven data distribution.\n",
      "\n",
      "Additionally, you can check the total number of slices by running the following query:\n",
      "\n",
      "```sql\n",
      "SELECT COUNT(DISTINCT slice) AS num_slices\n",
      "FROM stv_blocklist;\n",
      "```\n",
      "\n",
      "This will give you the total number of slices in your Amazon Redshift cluster. You can then compare the number of rows returned by the first query with the total number of slices to ensure that all slices are represented.\n"
     ]
    }
   ],
   "source": [
    "result = send_prompt(f\"\"\"For table STV_BLOCKLIST defined in markdown, write SQL query to shows whether or not table data is actually distributed over all slices.<markdown> {table3}  </markdown>\"\"\", max_token_count=2000)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f451e92b-ad54-4100-86c4-30e46233f49f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `STV_BLOCKLIST` system table in Amazon Redshift provides detailed information about the data blocks that make up the tables in a Redshift cluster. Each row in this table represents a specific data block, and the columns provide various metadata about that block. Understanding the information in this table can be important for several reasons:\n",
      "\n",
      "1. **Performance Optimization**: The `STV_BLOCKLIST` table can help identify potential performance bottlenecks related to data storage and access. For example, columns like `minvalue` and `maxvalue` can indicate if data is well-distributed across blocks, which can affect query performance. Additionally, columns like `pinned` and `on_disk` can provide insights into memory usage and disk I/O patterns.\n",
      "\n",
      "2. **Data Distribution Analysis**: By examining the `num_values` column, you can determine how evenly data is distributed across blocks. Skewed data distribution can lead to query performance issues, and this table can help identify such imbalances.\n",
      "\n",
      "3. **Storage Management**: Columns like `temporary` and `preferred_diskno` can assist in storage management tasks. For instance, you can identify and potentially reclaim space used by temporary data blocks or ensure that data blocks are placed on the appropriate disks.\n",
      "\n",
      "4. **Troubleshooting**: Several columns in the `STV_BLOCKLIST` table, such as `modified`, `hdr_modified`, and `unsorted`, can provide clues about the state of the data blocks, which can be helpful when troubleshooting issues related to data corruption or query performance problems.\n",
      "\n",
      "5. **Internal Diagnostics**: While some columns like `extended_limits` and `tombstone` are primarily used for internal Amazon Redshift purposes, they can still provide valuable insights for advanced troubleshooting or when working with Amazon Redshift support.\n",
      "\n",
      "By analyzing the information in the `STV_BLOCKLIST` table, database administrators and developers can gain a deeper understanding of how data is stored and accessed within their Redshift cluster. This knowledge can help them optimize query performance, manage storage more effectively, and troubleshoot issues more efficiently.\n"
     ]
    }
   ],
   "source": [
    "result = send_prompt(f\"\"\"For Redshift system table STV_BLOCKLIST defined in markdown, tell the story about what is in the table and why is this important.<markdown> {table3}  </markdown>\"\"\", max_token_count=2000)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fe603-3c90-4792-af59-365af6323e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
